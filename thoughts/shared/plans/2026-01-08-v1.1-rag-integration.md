# v1.1 RAG Integration Implementation Plan

## Overview

**Goal:** Augment ATLASsemi's 8D analysis with semantic search over semiconductor reference documentation.

**What we're building:**
- Document indexing system (PDF, PowerPoint, Word + OCR)
- Three-tier taxonomy (domain-specific, universal methodologies, cross-cutting support)
- Semantic search with ChromaDB
- Mode-aware query filtering
- Integration with Analysis Agent

**Business Value:**
- Enriches 8D analysis with reference material (manuals, SOPs, standards)
- Reduces reliance on LLM parametric knowledge alone
- Enables continuous improvement as documentation grows

**Timeline:** ~12-17 hours across 4 phases

---

## Current State Analysis

**What exists:**
- ✅ All 4 agents implemented (Phases 0-3)
- ✅ WorkflowOrchestrator chains agents
- ✅ Analysis Agent has clear prompt structure for context injection
- ✅ Security tier enforcement (Tier 1/2/3)
- ✅ `src/atlassemi/knowledge/` directory created (empty)
- ✅ Dependencies anticipated in `requirements.txt`:
  - `chromadb>=0.4.0`
  - `sentence-transformers>=2.2.0`
  - `langchain>=0.1.0`

**What's missing:**
- ❌ Document indexing pipeline
- ❌ Text extraction for PDF/PowerPoint/Word
- ❌ OCR for images in documents
- ❌ ChromaDB collection management
- ❌ Metadata tagging system (three-tier taxonomy)
- ❌ Query interface with filtering
- ❌ RAG context injection into Analysis Agent

**Key Discoveries:**
- Analysis Agent prompt structure (analysis_agent.py:82-179) has clear insertion point for RAG context before "Your Task:" section
- PyMuPDF4LLM (2025 recommendation) specifically optimized for RAG chunk quality
- Tesseract OCR integrated directly into PyMuPDF (no separate library needed)
- OpenAI embeddings acceptable for Tier 1 (public docs, cloud-safe)

---

## Desired End State

**After v1.1 Complete:**

1. **Document Indexer working:**
   ```bash
   python -m atlassemi.knowledge.indexer \
     --source-dir reference_docs/ \
     --collection tier1_generic \
     --tier general_llm
   ```
   - Processes PDF, PowerPoint, Word files
   - Extracts text + OCR from images
   - Auto-tags with three-tier taxonomy
   - Stores in ChromaDB with metadata

2. **Query Engine working:**
   ```python
   from atlassemi.knowledge.query_engine import RAGQueryEngine

   engine = RAGQueryEngine(collection="tier1_generic")
   results = engine.query(
       query="How to detect Cpk degradation early?",
       mode=ProblemMode.EXCURSION,  # Mode-aware filtering
       top_k=10
   )
   ```

3. **Analysis Agent enriched:**
   - Automatically queries RAG before analysis
   - Injects top 10 relevant chunks into prompt
   - Enriched 8D analysis references specific manuals/SOPs

4. **Manual quality verified:**
   - Human evaluates: "Are retrieved chunks relevant?"
   - Human evaluates: "Does enriched analysis cite references appropriately?"
   - Human evaluates: "Is analysis quality better than without RAG?"

**Verification:**
- [ ] Indexer processes 10 sample documents successfully
- [ ] Query engine returns relevant results for test queries
- [ ] Analysis Agent produces enriched output with citations
- [ ] Manual testing confirms quality improvement

---

## What We're NOT Doing (v1.1 Scope Limits)

**Deferred to v1.2 (Database Pipeline):**
- ❌ SQL Server lineage tracing
- ❌ Power BI report debugging
- ❌ Stored procedure code search

**Deferred to v1.3 (Vision + Confidential):**
- ❌ Vision processing (LLaVA, GPT-4V)
- ❌ Image understanding for diagrams
- ❌ Tier 2/3 confidential document handling
- ❌ Project lifecycle management (active → archived → deleted)

**Deferred to Future:**
- ❌ Historical troubleshooting database
- ❌ Knowledge graph integration
- ❌ Multi-document cross-referencing
- ❌ Automatic citation generation

**Out of Scope Entirely:**
- ❌ Web scraping for documentation
- ❌ Real-time document updates
- ❌ Collaborative document annotation
- ❌ Multi-language support

---

## Implementation Approach

**Strategy:** Plan-driven (NOT TDD)

**Rationale:**
- RAG is exploratory (chunk size, relevance scoring need tuning)
- Quality is subjective (requires human evaluation)
- Similar to orchestrator v1.0 (plan-driven worked well)

**Phase Structure:**
1. **Phase 1:** Document Indexer (text extraction, OCR, tagging, ChromaDB storage)
2. **Phase 2:** Query Engine (semantic search, filtering, mode-aware boosting)
3. **Phase 3:** Analysis Agent Integration (context injection, testing)
4. **Phase 4:** Testing & Documentation (unit tests, integration tests, manual evaluation)

**Tech Stack:**
- **Document Processing:** pymupdf4llm (PDFs), python-pptx (PowerPoint), python-docx (Word), Tesseract OCR (via PyMuPDF)
- **Embeddings:** OpenAI text-embedding-3-small (Tier 1 safe for public docs)
- **Vector Store:** ChromaDB (local, persistent)
- **Chunking:** pymupdf4llm built-in (optimized for RAG)
- **Metadata:** Custom three-tier taxonomy

**Design Principles:**
- Folder-based organization (low friction)
- Content-based auto-tagging (smart defaults)
- Manual override with .meta.yaml (when needed)
- Incremental refinement (start simple, improve iteratively)

---

## Phase 1: Document Indexer

### Overview

Create document indexing pipeline that:
- Processes PDF, PowerPoint, Word files
- Extracts text content
- Applies OCR to images
- Tags documents with three-tier taxonomy
- Stores chunks in ChromaDB with metadata

**Folder-based organization:**
```
reference_docs/
├── methodologies/     # Tier 2: Universal (8D, DMAIC, 5 Whys)
├── yield/             # Tier 1: Domain-specific yield
├── operations/        # Tier 1: Domain-specific operations
├── organizational/    # Tier 1: Domain-specific organizational
└── support/           # Tier 3: Cross-cutting (statistics, communication)
```

### Changes Required

#### 1. Document Processor Base Class

**File:** `src/atlassemi/knowledge/document_processor.py`

```python
"""
Document processor base class and implementations for different file types.
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any
from pathlib import Path
from dataclasses import dataclass


@dataclass
class DocumentChunk:
    """A single chunk of document text with metadata."""

    text: str
    metadata: Dict[str, Any]
    page: int | None = None
    chunk_index: int = 0


class DocumentProcessor(ABC):
    """Base class for document processors."""

    @abstractmethod
    def process(self, file_path: Path) -> List[DocumentChunk]:
        """
        Process document and return chunks with metadata.

        Args:
            file_path: Path to document file

        Returns:
            List of DocumentChunk objects
        """
        pass


class PDFProcessor(DocumentProcessor):
    """Process PDF files using pymupdf4llm."""

    def __init__(self):
        try:
            import pymupdf4llm
            self.pymupdf = pymupdf4llm
        except ImportError:
            raise ImportError("pymupdf4llm not installed: pip install pymupdf4llm")

    def process(self, file_path: Path) -> List[DocumentChunk]:
        """Extract text from PDF with OCR for images."""
        # pymupdf4llm automatically handles OCR via Tesseract
        # Returns markdown-formatted text optimized for RAG
        md_text = self.pymupdf.to_markdown(str(file_path))

        # Split into reasonable chunks (pymupdf4llm handles this smartly)
        # For now, use simple paragraph splitting
        chunks = []
        paragraphs = md_text.split('\n\n')

        for idx, para in enumerate(paragraphs):
            if len(para.strip()) > 50:  # Skip tiny fragments
                chunk = DocumentChunk(
                    text=para.strip(),
                    metadata={
                        "source": file_path.name,
                        "file_type": "pdf",
                        "chunk_index": idx
                    },
                    chunk_index=idx
                )
                chunks.append(chunk)

        return chunks


class PowerPointProcessor(DocumentProcessor):
    """Process PowerPoint files using python-pptx."""

    def __init__(self):
        try:
            from pptx import Presentation
            self.Presentation = Presentation
        except ImportError:
            raise ImportError("python-pptx not installed: pip install python-pptx")

    def process(self, file_path: Path) -> List[DocumentChunk]:
        """Extract text from PowerPoint slides."""
        prs = self.Presentation(str(file_path))
        chunks = []

        for slide_num, slide in enumerate(prs.slides):
            slide_text = []

            # Extract text from all shapes
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    slide_text.append(shape.text)

            # Combine slide text
            text = "\n".join(slide_text).strip()

            if len(text) > 20:  # Skip empty slides
                chunk = DocumentChunk(
                    text=text,
                    metadata={
                        "source": file_path.name,
                        "file_type": "pptx",
                        "slide": slide_num + 1,
                        "chunk_index": slide_num
                    },
                    page=slide_num + 1,
                    chunk_index=slide_num
                )
                chunks.append(chunk)

        return chunks


class WordProcessor(DocumentProcessor):
    """Process Word documents using python-docx."""

    def __init__(self):
        try:
            from docx import Document
            self.Document = Document
        except ImportError:
            raise ImportError("python-docx not installed: pip install python-docx")

    def process(self, file_path: Path) -> List[DocumentChunk]:
        """Extract text from Word document."""
        doc = self.Document(str(file_path))
        chunks = []

        # Process paragraphs
        current_chunk = []
        chunk_index = 0

        for para in doc.paragraphs:
            text = para.text.strip()
            if text:
                current_chunk.append(text)

                # Create chunk every 3-5 paragraphs
                if len(current_chunk) >= 3:
                    chunk = DocumentChunk(
                        text="\n\n".join(current_chunk),
                        metadata={
                            "source": file_path.name,
                            "file_type": "docx",
                            "chunk_index": chunk_index
                        },
                        chunk_index=chunk_index
                    )
                    chunks.append(chunk)
                    current_chunk = []
                    chunk_index += 1

        # Add remaining chunk
        if current_chunk:
            chunk = DocumentChunk(
                text="\n\n".join(current_chunk),
                metadata={
                    "source": file_path.name,
                    "file_type": "docx",
                    "chunk_index": chunk_index
                },
                chunk_index=chunk_index
            )
            chunks.append(chunk)

        return chunks


def get_processor(file_path: Path) -> DocumentProcessor:
    """Factory function to get appropriate processor for file type."""
    suffix = file_path.suffix.lower()

    if suffix == '.pdf':
        return PDFProcessor()
    elif suffix in ['.pptx', '.ppt']:
        return PowerPointProcessor()
    elif suffix in ['.docx', '.doc']:
        return WordProcessor()
    else:
        raise ValueError(f"Unsupported file type: {suffix}")
```

#### 2. Metadata Tagger (Three-Tier Taxonomy)

**File:** `src/atlassemi/knowledge/tagger.py`

```python
"""
Automatic tagging system for three-tier document taxonomy.
"""

from typing import Dict, List, Any
from pathlib import Path
from dataclasses import dataclass


# Hardcoded keyword rules for v1.1
KEYWORD_RULES = {
    # Tier 2: Methodologies (universal)
    "methodology": {
        "8d": ["8D", "8-D", "D0", "D1", "D2", "D3", "D4", "D5", "D6", "D7", "D8",
               "containment action", "root cause", "corrective action", "eight disciplines"],
        "dmaic": ["DMAIC", "Define", "Measure", "Analyze", "Improve", "Control",
                  "Six Sigma", "six sigma"],
        "5_whys": ["5 Whys", "five whys", "why-why analysis", "root cause analysis"],
        "fishbone": ["fishbone", "Ishikawa", "cause and effect diagram"],
        "fmea": ["FMEA", "failure mode", "failure modes and effects"]
    },

    # Tier 1: Domain-specific
    "yield": ["Cpk", "Cp", "defect density", "yield loss", "excursion", "SPC",
              "defect", "wafer", "die", "parametric", "electrical test"],
    "operations": ["cycle time", "WIP", "work in progress", "throughput",
                   "Little's Law", "bottleneck", "utilization", "factory physics",
                   "queue time", "load factor"],
    "organizational": ["turnover", "retention", "culture", "engagement",
                       "morale", "attrition", "hiring", "onboarding"],
    "equipment": ["tool", "chamber", "preventive maintenance", "PM", "downtime",
                  "MTBF", "MTTR", "availability"],

    # Tier 3: Supporting knowledge
    "statistics": ["standard deviation", "sigma", "confidence interval", "hypothesis test",
                   "regression", "correlation", "distribution", "normal distribution"],
    "communication": ["presentation", "stakeholder", "communication plan", "escalation"],
    "project_management": ["project plan", "milestone", "deliverable", "timeline", "Gantt"]
}


@dataclass
class DocumentTags:
    """Tags for a document."""

    type: str  # "domain_knowledge", "methodology", "supporting_knowledge"
    applicability: str  # "specific", "universal", "cross_domain"
    domain: str | None = None  # For Tier 1
    subdomain: str | None = None
    topics: List[str] = None
    methodology_name: str | None = None  # For Tier 2
    applies_to_domains: List[str] = None
    applies_to_modes: List[str] = None
    knowledge_area: str | None = None  # For Tier 3

    def __post_init__(self):
        if self.topics is None:
            self.topics = []
        if self.applies_to_domains is None:
            self.applies_to_domains = []
        if self.applies_to_modes is None:
            self.applies_to_modes = []


class MetadataTagger:
    """Tags documents based on folder location and content analysis."""

    def __init__(self):
        self.keyword_rules = KEYWORD_RULES

    def tag_document(self, file_path: Path, text: str) -> DocumentTags:
        """
        Tag document based on folder location and content.

        Args:
            file_path: Path to document (folder provides hint)
            text: Full document text

        Returns:
            DocumentTags with appropriate taxonomy
        """
        # Step 1: Folder-based hint
        folder_name = file_path.parent.name.lower()

        # Step 2: Content analysis
        text_lower = text.lower()

        # Check for methodologies (Tier 2)
        for methodology, keywords in self.keyword_rules["methodology"].items():
            if any(kw.lower() in text_lower for kw in keywords):
                return DocumentTags(
                    type="methodology",
                    applicability="universal",
                    methodology_name=methodology,
                    applies_to_domains=["all"],
                    applies_to_modes=["excursion", "improvement", "operations"]
                )

        # Check for supporting knowledge (Tier 3)
        if folder_name == "support":
            knowledge_area = self._detect_knowledge_area(text_lower)
            return DocumentTags(
                type="supporting_knowledge",
                applicability="cross_domain",
                knowledge_area=knowledge_area
            )

        # Default to domain knowledge (Tier 1)
        domain = self._detect_domain(folder_name, text_lower)
        topics = self._detect_topics(text_lower, domain)

        return DocumentTags(
            type="domain_knowledge",
            applicability="specific",
            domain=domain,
            topics=topics
        )

    def _detect_domain(self, folder_name: str, text: str) -> str:
        """Detect domain from folder name or content."""
        # Folder name as primary hint
        if "yield" in folder_name:
            return "yield"
        elif "operation" in folder_name:
            return "operations"
        elif "organization" in folder_name:
            return "organizational"
        elif "equipment" in folder_name or "tool" in folder_name:
            return "equipment"

        # Content-based detection
        for domain, keywords in {k: v for k, v in self.keyword_rules.items()
                                 if k not in ["methodology"]}.items():
            if isinstance(keywords, list) and any(kw in text for kw in keywords):
                return domain

        return "general"

    def _detect_topics(self, text: str, domain: str) -> List[str]:
        """Extract specific topics from text based on domain."""
        topics = []

        if domain == "yield":
            if "cpk" in text or "cp" in text:
                topics.append("statistical_process_control")
            if "defect" in text:
                topics.append("defect_analysis")
            if "excursion" in text:
                topics.append("excursion_response")

        elif domain == "operations":
            if "cycle time" in text:
                topics.append("cycle_time")
            if "bottleneck" in text:
                topics.append("bottleneck_analysis")
            if "factory physics" in text:
                topics.append("factory_physics")

        return topics

    def _detect_knowledge_area(self, text: str) -> str:
        """Detect knowledge area for Tier 3 support documents."""
        if any(kw in text for kw in self.keyword_rules["statistics"]):
            return "statistics"
        elif any(kw in text for kw in self.keyword_rules["communication"]):
            return "communication"
        elif any(kw in text for kw in self.keyword_rules["project_management"]):
            return "project_management"
        else:
            return "general_support"
```

#### 3. Main Indexer Script

**File:** `src/atlassemi/knowledge/indexer.py`

```python
"""
Document indexing system for ATLASsemi RAG.

Usage:
    python -m atlassemi.knowledge.indexer \\
        --source-dir reference_docs/ \\
        --collection tier1_generic \\
        --tier general_llm
"""

import argparse
from pathlib import Path
from typing import List, Optional
import chromadb
from chromadb.config import Settings
from openai import OpenAI

from .document_processor import get_processor, DocumentChunk
from .tagger import MetadataTagger, DocumentTags


class DocumentIndexer:
    """Index documents into ChromaDB with three-tier taxonomy."""

    def __init__(self, collection_name: str, persist_directory: str = "./chroma_db"):
        """
        Initialize indexer.

        Args:
            collection_name: Name of ChromaDB collection
            persist_directory: Where to store ChromaDB data
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory

        # Initialize ChromaDB
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )

        # Get or create collection
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"description": "ATLASsemi reference documents"}
        )

        # Initialize tagger
        self.tagger = MetadataTagger()

        # Initialize OpenAI for embeddings (Tier 1 safe)
        self.openai_client = OpenAI()

    def index_directory(self, source_dir: Path, tier: str = "general_llm"):
        """
        Index all documents in a directory.

        Args:
            source_dir: Root directory containing documents
            tier: Security tier (general_llm, confidential_fab, top_secret)
        """
        # Find all supported files
        supported_extensions = ['.pdf', '.pptx', '.ppt', '.docx', '.doc']
        files = []
        for ext in supported_extensions:
            files.extend(source_dir.rglob(f'*{ext}'))

        print(f"Found {len(files)} documents to index")

        for file_path in files:
            try:
                print(f"\nProcessing: {file_path.name}")
                self._index_document(file_path, tier)
            except Exception as e:
                print(f"ERROR processing {file_path.name}: {e}")

    def _index_document(self, file_path: Path, tier: str):
        """Index a single document."""
        # Step 1: Process document (extract text, OCR, chunk)
        processor = get_processor(file_path)
        chunks = processor.process(file_path)
        print(f"  Extracted {len(chunks)} chunks")

        # Step 2: Combine all text for tagging
        full_text = "\n\n".join(chunk.text for chunk in chunks)

        # Step 3: Tag document with three-tier taxonomy
        tags = self.tagger.tag_document(file_path, full_text)
        print(f"  Tagged as: {tags.type} (applicability={tags.applicability})")

        # Step 4: Create embeddings
        embeddings = self._create_embeddings([chunk.text for chunk in chunks])

        # Step 5: Prepare metadata for ChromaDB
        ids = []
        documents = []
        metadatas = []

        for idx, chunk in enumerate(chunks):
            chunk_id = f"{file_path.stem}_{chunk.chunk_index}"
            ids.append(chunk_id)
            documents.append(chunk.text)

            # Combine chunk metadata with document tags
            metadata = {
                "source": chunk.metadata["source"],
                "file_type": chunk.metadata["file_type"],
                "chunk_index": chunk.chunk_index,
                "tier": tier,

                # Three-tier taxonomy
                "type": tags.type,
                "applicability": tags.applicability,
            }

            # Add tier-specific metadata
            if tags.type == "methodology":
                metadata["methodology_name"] = tags.methodology_name
                metadata["applies_to_domains"] = ",".join(tags.applies_to_domains)
                metadata["applies_to_modes"] = ",".join(tags.applies_to_modes)
            elif tags.type == "domain_knowledge":
                metadata["domain"] = tags.domain
                if tags.topics:
                    metadata["topics"] = ",".join(tags.topics)
            elif tags.type == "supporting_knowledge":
                metadata["knowledge_area"] = tags.knowledge_area

            # Add page/slide if available
            if chunk.page:
                metadata["page"] = chunk.page

            metadatas.append(metadata)

        # Step 6: Add to ChromaDB
        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas
        )

        print(f"  ✓ Indexed {len(chunks)} chunks to ChromaDB")

    def _create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Create embeddings using OpenAI text-embedding-3-small."""
        response = self.openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [item.embedding for item in response.data]


def main():
    """CLI entry point."""
    parser = argparse.ArgumentParser(description="Index documents for ATLASsemi RAG")
    parser.add_argument("--source-dir", type=str, required=True,
                        help="Directory containing documents to index")
    parser.add_argument("--collection", type=str, default="tier1_generic",
                        help="ChromaDB collection name")
    parser.add_argument("--tier", type=str, default="general_llm",
                        choices=["general_llm", "confidential_fab", "top_secret"],
                        help="Security tier")
    parser.add_argument("--persist-dir", type=str, default="./chroma_db",
                        help="ChromaDB persistence directory")

    args = parser.parse_args()

    # Validate source directory
    source_dir = Path(args.source_dir)
    if not source_dir.exists():
        print(f"ERROR: Source directory not found: {source_dir}")
        return

    # Create indexer and process
    indexer = DocumentIndexer(
        collection_name=args.collection,
        persist_directory=args.persist_dir
    )

    indexer.index_directory(source_dir, tier=args.tier)
    print("\n✓ Indexing complete!")


if __name__ == "__main__":
    main()
```

#### 4. Update requirements.txt

**File:** `requirements.txt`

```python
# Add to existing requirements:

# Document processing (RAG v1.1)
pymupdf4llm>=0.0.1  # PDF processing optimized for RAG
python-pptx>=0.6.21  # PowerPoint processing
python-docx>=1.1.0  # Word document processing
pytesseract>=0.3.10  # OCR wrapper (requires Tesseract binary)
```

#### 5. Update package __init__.py

**File:** `src/atlassemi/knowledge/__init__.py`

```python
"""
Knowledge management for ATLASsemi.

Includes:
- Document indexing (PDF, PowerPoint, Word + OCR)
- Semantic search with ChromaDB
- Three-tier taxonomy (domain/methodology/support)
"""

from .indexer import DocumentIndexer
from .document_processor import DocumentProcessor, DocumentChunk, get_processor
from .tagger import MetadataTagger, DocumentTags

__all__ = [
    "DocumentIndexer",
    "DocumentProcessor",
    "DocumentChunk",
    "get_processor",
    "MetadataTagger",
    "DocumentTags",
]
```

### Success Criteria

#### Automated Verification:
- [ ] Code formatting passes: `black src/atlassemi/knowledge/ && isort src/atlassemi/knowledge/`
- [ ] Type checking passes: `mypy src/atlassemi/knowledge/`
- [ ] No linting errors: `flake8 src/atlassemi/knowledge/`
- [ ] Indexer runs without errors: `python -m atlassemi.knowledge.indexer --source-dir test_docs/ --collection test`

#### Manual Verification:
- [ ] Create test `reference_docs/` folder with structure (methodologies/, yield/, operations/, support/)
- [ ] Place 3-5 sample PDFs in folders
- [ ] Run indexer and verify ChromaDB populated
- [ ] Check ChromaDB metadata: `chroma_db/` directory created with data
- [ ] Verify tags are correct for each document type
- [ ] Test OCR: Include scanned PDF, verify text extracted

**Implementation Note:** After completing this phase and all automated verification passes, pause for manual testing confirmation before proceeding to Phase 2.

---

## Phase 2: Query Engine

### Overview

Create semantic search query engine that:
- Queries ChromaDB with mode-aware filtering
- Implements three-tier taxonomy strategy
- Returns top-k relevant chunks with metadata
- Supports single-tier or multi-tier queries

**Query Strategy:**
- Always include Tier 2 (methodologies) - universal
- Always include Tier 3 (supporting knowledge) - broad
- Dynamically select Tier 1 based on problem mode

### Changes Required

#### 1. Query Engine

**File:** `src/atlassemi/knowledge/query_engine.py`

```python
"""
Semantic search query engine for ATLASsemi RAG.
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import chromadb
from chromadb.config import Settings
from openai import OpenAI

from ..agents.base import ProblemMode


@dataclass
class RAGResult:
    """A single RAG search result."""

    text: str
    metadata: Dict[str, Any]
    score: float  # Relevance score

    def format_citation(self) -> str:
        """Format as citation for prompt."""
        source = self.metadata.get("source", "Unknown")
        page = self.metadata.get("page")

        citation = f"[{source}"
        if page:
            citation += f", p.{page}"
        citation += "]"

        return f"{citation}\n{self.text}"


class RAGQueryEngine:
    """Query engine for semantic search over indexed documents."""

    def __init__(self, collection_name: str, persist_directory: str = "./chroma_db"):
        """
        Initialize query engine.

        Args:
            collection_name: Name of ChromaDB collection
            persist_directory: Where ChromaDB data is stored
        """
        self.collection_name = collection_name

        # Initialize ChromaDB
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )

        # Get collection
        try:
            self.collection = self.client.get_collection(name=collection_name)
        except Exception as e:
            raise ValueError(f"Collection '{collection_name}' not found. "
                             f"Run indexer first: {e}")

        # Initialize OpenAI for query embeddings
        self.openai_client = OpenAI()

    def query(
        self,
        query: str,
        mode: Optional[ProblemMode] = None,
        top_k: int = 10,
        tier: Optional[str] = None
    ) -> List[RAGResult]:
        """
        Query for relevant documents with mode-aware filtering.

        Args:
            query: Search query text
            mode: Problem mode (for domain-specific filtering)
            top_k: Number of results to return
            tier: Filter by security tier (optional)

        Returns:
            List of RAGResult objects
        """
        # Create query embedding
        query_embedding = self._create_embedding(query)

        # Build filter based on three-tier strategy
        where_filter = self._build_filter(mode, tier)

        # Query ChromaDB
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            where=where_filter,
            include=["documents", "metadatas", "distances"]
        )

        # Convert to RAGResult objects
        rag_results = []
        if results["documents"] and results["documents"][0]:
            for text, metadata, distance in zip(
                results["documents"][0],
                results["metadatas"][0],
                results["distances"][0]
            ):
                # Convert distance to similarity score (1 - distance)
                score = 1.0 - min(distance, 1.0)

                rag_results.append(RAGResult(
                    text=text,
                    metadata=metadata,
                    score=score
                ))

        return rag_results

    def _build_filter(
        self,
        mode: Optional[ProblemMode],
        tier: Optional[str]
    ) -> Optional[Dict[str, Any]]:
        """
        Build ChromaDB where filter based on three-tier strategy.

        Three-tier strategy:
        - Always include Tier 2 (methodologies) - universal
        - Always include Tier 3 (supporting knowledge) - broad
        - Dynamically include Tier 1 based on mode
        """
        filters = []

        # Tier filter
        if tier:
            filters.append({"tier": tier})

        # Three-tier taxonomy filter
        # Always include Tier 2 and 3
        type_filters = [
            {"type": "methodology"},  # Tier 2: Universal
            {"type": "supporting_knowledge"}  # Tier 3: Cross-cutting
        ]

        # Add Tier 1 based on mode
        if mode == ProblemMode.EXCURSION:
            # Prioritize yield domain
            type_filters.append({"domain": "yield"})
            type_filters.append({"domain": "equipment"})
        elif mode == ProblemMode.OPERATIONS:
            # Prioritize operations domain
            type_filters.append({"domain": "operations"})
        elif mode == ProblemMode.IMPROVEMENT:
            # Balanced - include both
            type_filters.append({"domain": "yield"})
            type_filters.append({"domain": "operations"})
        else:
            # No mode specified - include all domain knowledge
            type_filters.append({"type": "domain_knowledge"})

        # Combine with OR logic
        if type_filters:
            filters.append({"$or": type_filters})

        # Combine all filters with AND logic
        if len(filters) == 1:
            return filters[0]
        elif len(filters) > 1:
            return {"$and": filters}
        else:
            return None

    def _create_embedding(self, text: str) -> List[float]:
        """Create embedding for query text."""
        response = self.openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding


def format_rag_context(results: List[RAGResult]) -> str:
    """
    Format RAG results as context for LLM prompt.

    Args:
        results: List of RAG search results

    Returns:
        Formatted string with citations
    """
    if not results:
        return ""

    context = "**Reference Material:**\n\n"

    for idx, result in enumerate(results, 1):
        context += f"{idx}. {result.format_citation()}\n\n"

    context += "---\n\n"

    return context
```

#### 2. Update knowledge package __init__.py

**File:** `src/atlassemi/knowledge/__init__.py`

```python
"""
Knowledge management for ATLASsemi.
"""

from .indexer import DocumentIndexer
from .document_processor import DocumentProcessor, DocumentChunk, get_processor
from .tagger import MetadataTagger, DocumentTags
from .query_engine import RAGQueryEngine, RAGResult, format_rag_context

__all__ = [
    "DocumentIndexer",
    "DocumentProcessor",
    "DocumentChunk",
    "get_processor",
    "MetadataTagger",
    "DocumentTags",
    "RAGQueryEngine",
    "RAGResult",
    "format_rag_context",
]
```

### Success Criteria

#### Automated Verification:
- [ ] Code formatting passes: `black src/atlassemi/knowledge/query_engine.py`
- [ ] Type checking passes: `mypy src/atlassemi/knowledge/query_engine.py`
- [ ] No linting errors: `flake8 src/atlassemi/knowledge/query_engine.py`
- [ ] Can import and instantiate: `python -c "from atlassemi.knowledge import RAGQueryEngine; RAGQueryEngine('test')"`

#### Manual Verification:
- [ ] Test query for yield problem returns yield + methodology + support docs
- [ ] Test query for operations problem returns operations + methodology + support docs
- [ ] Test query with no mode returns all types
- [ ] Verify relevance scores are reasonable (>0.5 for good matches)
- [ ] Verify citations format correctly with source and page numbers
- [ ] Test with 5 different queries, evaluate if top 3 results are relevant

**Implementation Note:** After automated tests pass, conduct manual quality evaluation of search results before proceeding to Phase 3.

---

## Phase 3: Analysis Agent Integration

### Overview

Integrate RAG query engine with Analysis Agent:
- Query RAG before 8D analysis
- Inject top-k relevant chunks into prompt
- Test enriched analysis quality
- Verify citations appear in output

### Changes Required

#### 1. Modify Analysis Agent to accept RAG context

**File:** `src/atlassemi/agents/analysis_agent.py`

**Changes:**

Add RAG integration to `generate_prompt()` method:

```python
# After line 99 (before "**Your Task:**"), insert:

        # RAG Context (if available)
        rag_context = agent_input.context.get('rag_context', '')
        if rag_context:
            prompt += f"\n{rag_context}\n"
```

Full updated method signature at line 62:

```python
    def generate_prompt(self, agent_input: AgentInput) -> str:
        """
        Generate 8D analysis prompt with optional RAG context.

        Args:
            agent_input: Contains narrative + clarifications + optional rag_context

        Returns:
            Prompt for comprehensive 8D analysis
        """
        mode = agent_input.mode
        narrative = agent_input.context.get('narrative', '')
        narrative_analysis = agent_input.context.get('narrative_analysis', {})
        clarifications = agent_input.context.get('clarifications', {})
        rag_context = agent_input.context.get('rag_context', '')  # NEW

        # Extract context
        observations = narrative_analysis.get('observations', [])
        suspected_causes = narrative_analysis.get('suspected_causes', [])
        constraints = narrative_analysis.get('constraints', [])

        prompt = f"""You are a semiconductor fab problem-solving expert conducting an 8D analysis.

**Problem Mode:** {mode.value}

**User Narrative:**
{narrative}

**Observations (Facts):**
{self._format_list(observations)}

**Suspected Causes (Hypotheses to validate):**
{self._format_list(suspected_causes)}

**Constraints:**
{self._format_list(constraints)}

**Clarifications:**
{self._format_clarifications(clarifications)}
"""

        # NEW: Insert RAG context if available
        if rag_context:
            prompt += f"\n{rag_context}\n"

        # Continue with existing "**Your Task:**" section...
        prompt += """
**Your Task:**

Conduct a structured 8D (Eight Disciplines) analysis of this problem.
...
"""

        return prompt
```

#### 2. Modify Orchestrator to query RAG before Analysis Agent

**File:** `src/atlassemi/orchestrator/workflow.py`

**Changes:**

Update `_execute_phase_2()` method to query RAG:

```python
# Around line 180-200, before calling analysis_agent.execute()

    def _execute_phase_2(
        self,
        narrative: str,
        narrative_analysis: dict,
        clarifications: dict,
        mode: ProblemMode,
        tier: SecurityTier
    ) -> tuple:
        """
        Execute Phase 2: 8D Analysis with RAG enrichment.
        """
        # NEW: Query RAG for relevant reference material
        rag_context = ""
        try:
            from atlassemi.knowledge import RAGQueryEngine, format_rag_context

            # Initialize query engine
            query_engine = RAGQueryEngine(collection_name="tier1_generic")

            # Create query from narrative + key facts
            query_text = self._create_rag_query(narrative, narrative_analysis)

            # Search for relevant chunks
            results = query_engine.query(
                query=query_text,
                mode=mode,
                top_k=10
            )

            # Format as context
            if results:
                rag_context = format_rag_context(results)
                print(f"  RAG: Retrieved {len(results)} relevant chunks")

        except Exception as e:
            # RAG is optional - don't fail if it's not available
            print(f"  RAG: Not available ({e})")

        # Create Analysis Agent
        analysis_agent = AnalysisAgent(model_router=self.model_router)

        # Build context with RAG
        context = {
            'narrative': narrative,
            'narrative_analysis': narrative_analysis,
            'clarifications': clarifications,
            'rag_context': rag_context  # NEW
        }

        # Execute analysis
        agent_input = AgentInput(
            mode=mode,
            security_tier=tier,
            context=context
        )

        analysis_output = analysis_agent.execute(agent_input)

        return analysis_output, analysis_output.cost_usd

    def _create_rag_query(self, narrative: str, narrative_analysis: dict) -> str:
        """Create search query from narrative and key observations."""
        # Extract key terms from narrative
        observations = narrative_analysis.get('observations', [])

        # Combine narrative with top observations
        query_parts = [narrative[:200]]  # First 200 chars of narrative
        if observations:
            query_parts.extend(observations[:3])  # Top 3 observations

        return " ".join(query_parts)
```

### Success Criteria

#### Automated Verification:
- [ ] Code formatting passes: `black src/atlassemi/agents/analysis_agent.py src/atlassemi/orchestrator/workflow.py`
- [ ] Type checking passes: `mypy src/atlassemi/agents/ src/atlassemi/orchestrator/`
- [ ] No linting errors: `flake8 src/atlassemi/agents/ src/atlassemi/orchestrator/`
- [ ] Orchestrator runs without errors (with and without ChromaDB available)

#### Manual Verification:
- [ ] Run full workflow with indexed documents: `python cli.py`
- [ ] Verify "RAG: Retrieved N chunks" message appears
- [ ] Check analysis output includes citations to reference material
- [ ] Compare analysis quality: with RAG vs without RAG
- [ ] Verify graceful degradation if RAG unavailable (no crash)
- [ ] Test with 3 different problem narratives, evaluate enrichment quality

**Implementation Note:** Conduct side-by-side comparison of analysis quality (with vs without RAG) before marking complete.

---

## Phase 4: Testing & Documentation

### Overview

Create comprehensive test suite and documentation:
- Unit tests for document processor
- Unit tests for metadata tagger
- Unit tests for query engine
- Integration test with Analysis Agent
- Manual quality evaluation checklist
- User documentation (indexing workflow, usage examples)

### Changes Required

#### 1. Unit Tests - Document Processor

**File:** `tests/test_document_processor.py`

```python
"""
Unit tests for document processing.
"""

import pytest
from pathlib import Path
from atlassemi.knowledge.document_processor import (
    PDFProcessor,
    PowerPointProcessor,
    WordProcessor,
    get_processor,
    DocumentChunk
)


def test_get_processor_pdf():
    """Test processor factory returns PDFProcessor for .pdf files."""
    processor = get_processor(Path("test.pdf"))
    assert isinstance(processor, PDFProcessor)


def test_get_processor_powerpoint():
    """Test processor factory returns PowerPointProcessor for .pptx files."""
    processor = get_processor(Path("test.pptx"))
    assert isinstance(processor, PowerPointProcessor)


def test_get_processor_word():
    """Test processor factory returns WordProcessor for .docx files."""
    processor = get_processor(Path("test.docx"))
    assert isinstance(processor, WordProcessor)


def test_get_processor_unsupported():
    """Test processor factory raises error for unsupported file types."""
    with pytest.raises(ValueError, match="Unsupported file type"):
        get_processor(Path("test.txt"))


# Mock-based tests for actual processing
@pytest.fixture
def mock_pdf_file(tmp_path):
    """Create mock PDF file for testing."""
    # Note: Actual test would use a real sample PDF
    # For unit test, we'd mock pymupdf4llm.to_markdown()
    return tmp_path / "test.pdf"


def test_pdf_processor_returns_chunks(mock_pdf_file, monkeypatch):
    """Test PDFProcessor returns list of DocumentChunk objects."""
    # Mock pymupdf4llm.to_markdown to return test data
    def mock_to_markdown(path):
        return "Test paragraph 1\n\nTest paragraph 2\n\nTest paragraph 3"

    processor = PDFProcessor()
    monkeypatch.setattr(processor.pymupdf, "to_markdown", mock_to_markdown)

    chunks = processor.process(mock_pdf_file)

    assert len(chunks) > 0
    assert all(isinstance(chunk, DocumentChunk) for chunk in chunks)
    assert all(chunk.metadata["file_type"] == "pdf" for chunk in chunks)
```

#### 2. Unit Tests - Metadata Tagger

**File:** `tests/test_tagger.py`

```python
"""
Unit tests for metadata tagging.
"""

import pytest
from pathlib import Path
from atlassemi.knowledge.tagger import MetadataTagger, DocumentTags


@pytest.fixture
def tagger():
    """Create MetadataTagger instance."""
    return MetadataTagger()


def test_tag_methodology_document(tagger):
    """Test tagging of 8D methodology document."""
    text = """
    8D Problem Solving Process

    D0: Preparation and Emergency Response Actions
    D1: Establish the Team
    D2: Describe the Problem
    D3: Develop Interim Containment Actions
    D4: Determine and Verify Root Causes
    """

    tags = tagger.tag_document(Path("methodologies/8d_handbook.pdf"), text)

    assert tags.type == "methodology"
    assert tags.applicability == "universal"
    assert tags.methodology_name == "8d"
    assert "all" in tags.applies_to_domains


def test_tag_yield_document(tagger):
    """Test tagging of yield-specific document."""
    text = """
    Statistical Process Control for Semiconductor Manufacturing

    Cpk and Cp calculation methods for yield monitoring.
    Defect density trending and excursion detection.
    """

    tags = tagger.tag_document(Path("yield/spc_handbook.pdf"), text)

    assert tags.type == "domain_knowledge"
    assert tags.applicability == "specific"
    assert tags.domain == "yield"
    assert "statistical_process_control" in tags.topics


def test_tag_operations_document(tagger):
    """Test tagging of operations-specific document."""
    text = """
    Factory Physics: Principles of Manufacturing

    Little's Law: WIP = Throughput × Cycle Time
    Bottleneck analysis and constraint management.
    """

    tags = tagger.tag_document(Path("operations/factory_physics.pdf"), text)

    assert tags.type == "domain_knowledge"
    assert tags.applicability == "specific"
    assert tags.domain == "operations"
    assert "factory_physics" in tags.topics


def test_tag_support_document(tagger):
    """Test tagging of supporting knowledge document."""
    text = """
    Statistical Methods for Engineers

    Standard deviation, confidence intervals, hypothesis testing.
    Normal distribution and regression analysis.
    """

    tags = tagger.tag_document(Path("support/statistics.pdf"), text)

    assert tags.type == "supporting_knowledge"
    assert tags.applicability == "cross_domain"
    assert tags.knowledge_area == "statistics"
```

#### 3. Unit Tests - Query Engine

**File:** `tests/test_query_engine.py`

```python
"""
Unit tests for RAG query engine.
"""

import pytest
from atlassemi.knowledge.query_engine import RAGQueryEngine, format_rag_context
from atlassemi.agents.base import ProblemMode


# Note: These tests require ChromaDB to be set up with test data
# For true unit tests, we'd mock ChromaDB calls


@pytest.fixture
def query_engine(tmp_path):
    """Create query engine with test collection."""
    # This would use a test ChromaDB collection
    return RAGQueryEngine(
        collection_name="test_collection",
        persist_directory=str(tmp_path / "chroma_test")
    )


def test_query_engine_mode_aware_filtering(query_engine):
    """Test that query engine filters by mode correctly."""
    # This test would require test data indexed
    results = query_engine.query(
        query="Cpk degradation",
        mode=ProblemMode.EXCURSION,
        top_k=5
    )

    # Results should include:
    # - Methodology documents (Tier 2)
    # - Yield domain documents (Tier 1)
    # - Support documents (Tier 3)
    assert len(results) >= 0  # May be 0 if no test data


def test_format_rag_context():
    """Test RAG context formatting for prompts."""
    from atlassemi.knowledge.query_engine import RAGResult

    results = [
        RAGResult(
            text="Test content from SPC handbook",
            metadata={"source": "spc_handbook.pdf", "page": 42},
            score=0.95
        ),
        RAGResult(
            text="8D methodology D4 root cause analysis",
            metadata={"source": "8d_guide.pdf", "page": 15},
            score=0.87
        )
    ]

    context = format_rag_context(results)

    assert "**Reference Material:**" in context
    assert "[spc_handbook.pdf, p.42]" in context
    assert "[8d_guide.pdf, p.15]" in context
    assert "Test content from SPC handbook" in context
```

#### 4. Integration Test

**File:** `tests/test_rag_integration.py`

```python
"""
Integration test for RAG with Analysis Agent.
"""

import pytest
from atlassemi.agents.analysis_agent import AnalysisAgent
from atlassemi.agents.base import AgentInput, ProblemMode, SecurityTier
from atlassemi.knowledge import RAGQueryEngine, format_rag_context


@pytest.mark.integration
def test_analysis_agent_with_rag():
    """
    Test that Analysis Agent can accept RAG context and produce enriched analysis.

    This is a manual test that requires:
    1. ChromaDB indexed with test documents
    2. Real LLM calls (or mock with pre-recorded responses)
    """
    # Step 1: Query RAG
    query_engine = RAGQueryEngine(collection_name="tier1_generic")
    results = query_engine.query(
        query="Cpk degradation on deposition tool",
        mode=ProblemMode.EXCURSION,
        top_k=5
    )

    rag_context = format_rag_context(results) if results else ""

    # Step 2: Create Analysis Agent
    agent = AnalysisAgent(model_router=None)  # Uses mock LLM

    # Step 3: Execute with RAG context
    agent_input = AgentInput(
        mode=ProblemMode.EXCURSION,
        security_tier=SecurityTier.GENERAL_LLM,
        context={
            "narrative": "Cpk dropped from 1.8 to 1.2 on Chamber B deposition",
            "narrative_analysis": {
                "observations": ["Cpk degradation", "Chamber B specific"],
                "suspected_causes": ["Tool drift", "Recipe issue"]
            },
            "clarifications": {},
            "rag_context": rag_context
        }
    )

    output = agent.execute(agent_input)

    # Verify RAG context was used
    assert output.content is not None
    # Would check if output references the RAG sources
```

#### 5. Documentation - Indexing Workflow

**File:** `docs/RAG_INDEXING.md`

```markdown
# RAG Document Indexing Guide

## Overview

ATLASsemi v1.1 includes document indexing for semantic search over semiconductor reference materials.

## Prerequisites

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Install Tesseract OCR** (for scanned documents):
   - **Windows:** Download from https://github.com/UB-Mannheim/tesseract/wiki
   - **Mac:** `brew install tesseract`
   - **Linux:** `sudo apt-get install tesseract-ocr`

3. **Set OpenAI API key** (for embeddings):
   ```bash
   export OPENAI_API_KEY="your-key-here"
   ```

## Document Organization

Create folder structure:

```
reference_docs/
├── methodologies/     # Tier 2: Universal problem-solving (8D, DMAIC, 5 Whys)
├── yield/             # Tier 1: Yield-specific (SPC, defect analysis)
├── operations/        # Tier 1: Operations-specific (Factory Physics, cycle time)
├── organizational/    # Tier 1: Organizational (retention, culture)
└── support/           # Tier 3: Cross-cutting (statistics, communication)
```

**Supported formats:**
- PDF (.pdf)
- PowerPoint (.pptx, .ppt)
- Word (.docx, .doc)

## Indexing Documents

### Basic Usage

```bash
python -m atlassemi.knowledge.indexer \
  --source-dir reference_docs/ \
  --collection tier1_generic \
  --tier general_llm
```

### Advanced Options

```bash
python -m atlassemi.knowledge.indexer \
  --source-dir /path/to/docs \
  --collection my_custom_collection \
  --tier general_llm \
  --persist-dir /path/to/chroma_db
```

**Parameters:**
- `--source-dir`: Directory containing documents to index
- `--collection`: ChromaDB collection name (default: `tier1_generic`)
- `--tier`: Security tier (default: `general_llm`)
- `--persist-dir`: ChromaDB storage location (default: `./chroma_db`)

## Automatic Tagging

Documents are automatically tagged based on:

1. **Folder location** (primary hint)
2. **Content analysis** (keyword detection)
3. **Manual override** (optional `.meta.yaml` files)

### Three-Tier Taxonomy

**Tier 1 - Domain-Specific:**
- Folder: `yield/`, `operations/`, `organizational/`
- Keywords: Cpk, defect, cycle time, WIP, turnover
- Tag: `type="domain_knowledge"`, `domain="yield"`

**Tier 2 - Universal Methodologies:**
- Folder: `methodologies/`
- Keywords: 8D, D0-D8, DMAIC, 5 Whys
- Tag: `type="methodology"`, `methodology_name="8d"`

**Tier 3 - Cross-Cutting Support:**
- Folder: `support/`
- Keywords: statistics, standard deviation, communication
- Tag: `type="supporting_knowledge"`, `knowledge_area="statistics"`

### Manual Override

Create `.meta.yaml` file alongside document:

**Example:** `reference_docs/yield/custom_doc.pdf.meta.yaml`

```yaml
type: domain_knowledge
domain: yield
subdomain: defect_classification
topics:
  - optical_defects
  - electrical_defects
```

## Verifying Indexed Data

Check ChromaDB contents:

```python
import chromadb

client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_collection("tier1_generic")

print(f"Total chunks: {collection.count()}")
```

## Troubleshooting

**Issue:** "Tesseract not found"
- **Solution:** Install Tesseract OCR binary, set `TESSDATA_PREFIX` environment variable

**Issue:** "OpenAI API key not set"
- **Solution:** `export OPENAI_API_KEY="your-key"`

**Issue:** "No documents found"
- **Solution:** Check folder structure, ensure files have supported extensions

## Next Steps

After indexing, RAG queries are automatically integrated into ATLASsemi's Analysis Agent. See `docs/RAG_USAGE.md` for query examples.
```

#### 6. Manual Quality Evaluation Checklist

**File:** `docs/RAG_QUALITY_CHECKLIST.md`

```markdown
# RAG Quality Evaluation Checklist

Use this checklist to evaluate RAG system quality after implementation.

## Indexing Quality

### Document Processing
- [ ] PDF text extraction works correctly
- [ ] PowerPoint slides extracted with proper formatting
- [ ] Word documents processed without errors
- [ ] OCR extracts text from scanned PDFs
- [ ] Images in documents are OCR'd
- [ ] Chunk sizes are reasonable (200-500 tokens)

### Metadata Tagging
- [ ] Methodology documents tagged as `type="methodology"`
- [ ] Yield documents tagged as `domain="yield"`
- [ ] Operations documents tagged as `domain="operations"`
- [ ] Support documents tagged as `type="supporting_knowledge"`
- [ ] Folder-based hints work correctly
- [ ] Content-based detection identifies 8D, DMAIC, etc.
- [ ] Topics extracted correctly (SPC, defect analysis, cycle time)

## Query Quality

### Relevance Testing

**Test 1: Yield Problem**
- Query: "Cpk degradation on deposition tool Chamber B"
- Mode: EXCURSION
- Expected: SPC documents, 8D methodology, yield-specific guides
- [ ] Top 3 results are relevant
- [ ] Mix of Tier 1 (yield), Tier 2 (methodology), Tier 3 (statistics)
- [ ] Relevance scores > 0.6

**Test 2: Operations Problem**
- Query: "Cycle time increased causing bottleneck at lithography"
- Mode: OPERATIONS
- Expected: Factory Physics, bottleneck analysis, 8D methodology
- [ ] Top 3 results are relevant
- [ ] Operations docs prioritized over yield docs
- [ ] Methodologies always included

**Test 3: Cross-Domain Problem**
- Query: "Yield loss affecting production throughput"
- Mode: IMPROVEMENT
- Expected: Both yield and operations docs
- [ ] Results span multiple domains
- [ ] Balanced mix of yield and operations
- [ ] Root cause analysis methods included

### Filter Testing
- [ ] Mode=EXCURSION filters correctly (yield focus)
- [ ] Mode=OPERATIONS filters correctly (operations focus)
- [ ] Mode=IMPROVEMENT includes both domains
- [ ] No mode specified returns all types
- [ ] Tier filter works (general_llm only)

## Integration Quality

### Analysis Agent Enrichment
- [ ] RAG context successfully injected into prompt
- [ ] Analysis output includes citations [source, p.X]
- [ ] Referenced material is actually relevant
- [ ] Analysis quality improves with RAG vs without
- [ ] Graceful degradation if RAG unavailable

### End-to-End Workflow
- [ ] Full workflow runs without errors
- [ ] "RAG: Retrieved N chunks" message appears
- [ ] Analysis references specific manuals/SOPs
- [ ] User can trace citations to original documents

## Subjective Quality Assessment

Rate on 1-5 scale:

### Relevance (Are retrieved chunks useful?)
- [ ] 1 - Not relevant at all
- [ ] 2 - Somewhat relevant
- [ ] 3 - Moderately relevant
- [ ] 4 - Very relevant
- [ ] 5 - Extremely relevant, directly answers question

### Coverage (Does RAG find what you expect?)
- [ ] 1 - Misses obvious documents
- [ ] 2 - Finds some relevant docs, misses others
- [ ] 3 - Finds most relevant docs
- [ ] 4 - Finds all relevant docs
- [ ] 5 - Finds all relevant docs + useful tangential material

### Citation Quality (Are citations formatted well?)
- [ ] 1 - Missing or broken citations
- [ ] 2 - Citations present but unclear
- [ ] 3 - Citations usable but could be better
- [ ] 4 - Citations clear and useful
- [ ] 5 - Citations excellent, easy to trace

## Performance Testing
- [ ] Indexing 100 docs completes in < 5 minutes
- [ ] Query response time < 2 seconds
- [ ] ChromaDB size reasonable (< 1GB for 1000 docs)
- [ ] OpenAI embedding costs reasonable (< $0.01 per 100 queries)

## Acceptance Criteria

**Minimum for v1.1 Release:**
- [ ] All indexing quality checks pass
- [ ] At least 4/5 relevance tests pass
- [ ] Integration quality checks pass
- [ ] Subjective ratings >= 3/5 on all dimensions
- [ ] Performance within acceptable limits
```

### Success Criteria

#### Automated Verification:
- [ ] All unit tests pass: `pytest tests/test_document_processor.py tests/test_tagger.py tests/test_query_engine.py -v`
- [ ] Code coverage for knowledge module: `pytest --cov=atlassemi.knowledge --cov-report=html`
- [ ] Test coverage >= 70% for new code
- [ ] Integration test passes: `pytest tests/test_rag_integration.py -v -m integration`
- [ ] All documentation files created and readable

#### Manual Verification:
- [ ] Work through `RAG_INDEXING.md`, verify all steps work
- [ ] Complete `RAG_QUALITY_CHECKLIST.md`, all items passing
- [ ] Test with 10 sample documents, evaluate quality
- [ ] Run 5 different queries, evaluate relevance manually
- [ ] Compare analysis quality: 3 problems with RAG vs without RAG
- [ ] Confirm subjective quality >= 3/5 on all dimensions

**Final Acceptance:**
- [ ] Human evaluation confirms RAG improves analysis quality
- [ ] Citations are useful and traceable
- [ ] System degrades gracefully if RAG unavailable
- [ ] Documentation sufficient for independent use

---

## Testing Strategy

### Unit Tests (Automated)

**Document Processor:**
- Test each file type processor (PDF, PowerPoint, Word)
- Mock file reading, test chunking logic
- Verify DocumentChunk objects created correctly
- Test error handling (corrupt files, missing files)

**Metadata Tagger:**
- Test folder-based tagging
- Test content-based keyword detection
- Test three-tier taxonomy classification
- Test edge cases (empty docs, mixed content)

**Query Engine:**
- Test mode-aware filtering
- Test three-tier query strategy
- Test result formatting and citations
- Mock ChromaDB, verify queries constructed correctly

### Integration Tests (Semi-Automated)

**RAG + Analysis Agent:**
- Index sample documents
- Run full workflow with RAG
- Verify enriched output includes citations
- Compare quality: with RAG vs without RAG

**Orchestrator + RAG:**
- Test RAG query creation from narrative
- Test graceful degradation if ChromaDB unavailable
- Verify cost tracking includes embedding costs

### Manual Testing (Human Evaluation)

**Quality Assessment:**
- [ ] Index 10 sample documents (mix of PDF, PowerPoint, Word)
- [ ] Run 5 different problem queries (yield, operations, mixed)
- [ ] Evaluate top 3 results: Are they relevant?
- [ ] Rate relevance, coverage, citation quality (1-5 scale)
- [ ] Compare analysis quality: 3 problems with RAG vs without

**Edge Cases:**
- [ ] Test with scanned PDF (OCR required)
- [ ] Test with PowerPoint containing mostly images
- [ ] Test with empty document
- [ ] Test with document in wrong folder (verify content-based override)
- [ ] Test query with no mode specified

---

## Performance Considerations

### Indexing Performance

**Expected:**
- PDF processing: ~2-5 seconds per page (depends on OCR)
- PowerPoint processing: ~0.5 seconds per slide
- Word processing: ~0.5 seconds per page
- Embedding generation: ~0.1 seconds per chunk (OpenAI API)

**Optimization:**
- Batch embeddings (100 chunks at a time)
- Process documents in parallel (future enhancement)
- Cache processed documents (skip if unchanged)

### Query Performance

**Expected:**
- Query embedding: ~0.1 seconds (OpenAI API)
- ChromaDB search: ~0.2-0.5 seconds (depends on collection size)
- Total query time: ~0.5-1.0 seconds

**Optimization:**
- Use local embeddings for Tier 2/3 (sentence-transformers)
- Limit top_k to 10 (more doesn't improve quality)
- Consider HNSW index tuning for large collections

### Cost Considerations

**OpenAI Embeddings:**
- Model: text-embedding-3-small
- Cost: $0.00002 per 1K tokens
- Typical document: 500 chunks × 200 tokens = 100K tokens = $0.002 per document
- Query: 100 tokens = $0.000002 per query

**ChromaDB Storage:**
- ~1KB per chunk (text + metadata)
- 1000 documents × 500 chunks = 500MB
- Acceptable for local deployment

---

## Migration Notes

**v1.0 → v1.1 (Adding RAG):**
- No breaking changes to existing agents
- RAG is optional - system works without it
- New dependencies: pymupdf4llm, python-pptx, python-docx, pytesseract
- New directory: chroma_db/ (add to .gitignore)

**Future (v1.2 Database Pipeline):**
- Separate collection for SQL code chunks
- Different metadata schema (code-specific)
- No conflicts with v1.1 RAG

---

## References

**Design Documents:**
- Original design: `thoughts/ledgers/CONTINUITY_CLAUDE-atlassemi-mvp.md` (lines 503-655)
- Three-tier taxonomy: Ledger lines 523-537
- Metadata schema: Ledger lines 539-571
- Query strategy: Ledger lines 573-600
- Document organization: Ledger lines 602-636

**External Documentation:**
- [PyMuPDF Documentation](https://pymupdf.readthedocs.io)
- [PyMuPDF4LLM for RAG](https://pymupdf.readthedocs.io/en/latest/recipes-ocr.html)
- [Python OCR Best Practices 2025](https://unstract.com/blog/evaluating-python-pdf-to-text-libraries/)
- [ChromaDB Documentation](https://docs.trychroma.com)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)

**Related Plans:**
- v1.0 Orchestrator: `thoughts/shared/plans/2026-01-07-orchestrator-test-suite-mvp.md`
- v1.2 Database Pipeline: (to be created for new data scientist)

---

**Plan Version:** 1.0
**Created:** 2026-01-08
**Estimated Effort:** 12-17 hours across 4 phases
**Dependencies:** v1.0 complete, Tesseract installed, OpenAI API key

**Next Action:** Validate this plan using validate-agent skill before implementation.
