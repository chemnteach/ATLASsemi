# v1.2 Database Pipeline Debugging Implementation Plan

## Overview

**Goal:** Build an automated system to trace Power BI reports back to SQL Server source code, enabling rapid debugging of data pipeline issues in semiconductor manufacturing analytics.

**Business Value:**
- **Current state:** Data pipeline tickets take 2-5 days to resolve (manual code review)
- **Target state:** Close tickets in 2-4 hours (automated tracing + semantic search)
- **Impact:** 5-10x faster resolution, higher production org velocity

**Use Case Example:**
```
Problem: "Yield Dashboard shows wrong Chamber B Cpk values"
Current process: Data scientist manually reviews 50+ stored procedures, traces JOINs
New process:
  1. Query: "Which stored procs populate Chamber B Cpk in yield dashboard?"
  2. System returns: sp_Calculate_Chamber_Metrics (line 342: JOIN mismatch)
  3. Root cause identified in <1 hour
```

## Current State Analysis

**What exists:**
- Power BI reports consuming data from SQL Server stored procedures
- Complex data pipelines with 100+ stored procedures
- Manual debugging: grep through code, trace table dependencies by hand
- No automated lineage tracking from report → dataset → query → table

**What's missing:**
- Automated Power BI → SQL Server lineage mapping
- Semantic search over SQL code ("find all procs that calculate Cpk")
- Dependency graph (which reports break if I change this table?)
- Integration with ATLASsemi for structured troubleshooting

**Key Constraints:**
- Must work with existing SQL Server infrastructure
- Power BI admin rights required for Scanner API
- Security: Confidential fab data (Tier 2) - local embeddings only
- Performance: Fast enough for interactive debugging (<5 seconds per query)

## Desired End State

After v1.2 implementation, a data scientist can:

1. **Trace report to source:**
   - Input: Power BI report name or measure
   - Output: Complete lineage (report → dataset → query → stored procs → tables)
   - Include line numbers and code snippets

2. **Find dependent reports:**
   - Input: SQL table or column name
   - Output: All Power BI reports that depend on it (impact analysis)

3. **Semantic code search:**
   - Input: Natural language query ("find procs that join Chamber table with Defect table")
   - Output: Ranked stored procedures with relevant code snippets

4. **Integrated troubleshooting:**
   - Use ATLASsemi with `ProblemMode.DATA_PIPELINE`
   - Analysis Agent queries lineage system automatically
   - Returns 8D analysis with root cause traced to specific SQL code

### Verification:
- [ ] Given "Yield Dashboard Report", system returns all source stored procedures in <3 seconds
- [ ] Given table "tblChamberMetrics", system returns all dependent reports
- [ ] Semantic search "calculate wafer yield" returns relevant stored procedures
- [ ] Integration test: ATLASsemi solves real production data ticket using pipeline context

## What We're NOT Doing

**Out of scope for v1.2:**
- ❌ Direct database schema changes (read-only metadata extraction)
- ❌ Automated SQL code fixes (human reviews suggested changes)
- ❌ Real-time monitoring or alerting (batch metadata refresh is sufficient)
- ❌ Support for non-SQL Server databases (Oracle, MySQL, etc.)
- ❌ Power BI report creation or modification
- ❌ Historical change tracking (focus on current state)
- ❌ Cross-database joins or multi-server lineage (single SQL Server instance)

**Deferred to later versions:**
- Vision processing for Power BI visuals (v1.3+)
- Historical troubleshooting database (v2.0+)
- Knowledge graph integration (v2.0+)

## Implementation Approach

**High-level strategy:**

1. **Metadata Extraction (Phases 1-2):** Pull metadata from SQL Server and Power BI into structured format
2. **Code Understanding (Phase 3):** Embed SQL code for semantic search
3. **Dependency Mapping (Phase 4):** Build queryable dependency graph
4. **Integration (Phase 5):** Connect to ATLASsemi Analysis Agent
5. **Validation (Phase 6):** Test with real production tickets

**Technology Stack:**

| Component | Technology | Rationale |
|-----------|-----------|-----------|
| SQL Metadata | SQL Server DMVs (sys.dm_sql_referenced_entities, sys.sql_modules) | Native, comprehensive, no extra tools needed |
| Power BI Lineage | Power BI Admin Scanner API | Official Microsoft API, complete lineage |
| Code Embeddings | SQL Server 2025 vectors + OpenAI fallback | Native vector support in SQL Server 2025, OpenAI for compatibility |
| Vector Storage | ChromaDB (local) | Consistent with v1.1 RAG, local-only for security |
| Dependency Graph | Structured metadata (JSON/relational) | Start simple, evaluate Neo4j/Memgraph if performance needed |
| Integration | New `ProblemMode.DATA_PIPELINE` in ATLASsemi | Extends existing architecture cleanly |

**Security Tier:** Confidential Fab (Tier 2)
- All embeddings use local models (no cloud APIs for SQL code)
- Metadata stays on-prem
- ChromaDB collection is project-specific, auto-deleted after investigation

---

## Phase 1: SQL Server Metadata Extraction

### Overview

Extract comprehensive metadata from SQL Server: schemas, tables, columns, stored procedures, views, functions, and dependencies.

**Goal:** Build a complete catalog of SQL Server objects with dependency relationships.

**Duration:** 4-6 hours

### Changes Required

#### 1. Metadata Extractor Module

**File:** `src/atlassemi/pipeline/sql_metadata_extractor.py`

**Purpose:** Query SQL Server system tables and DMVs to extract metadata.

```python
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
import pyodbc
from pathlib import Path


@dataclass
class SQLObject:
    """Represents a SQL Server database object."""
    object_id: int
    name: str
    schema_name: str
    object_type: str  # 'table', 'view', 'procedure', 'function'
    definition: Optional[str] = None  # SQL code for procs/views
    created_date: Optional[str] = None
    modified_date: Optional[str] = None


@dataclass
class SQLDependency:
    """Represents a dependency between SQL objects."""
    referencing_object_id: int
    referencing_object_name: str
    referenced_object_id: int
    referenced_object_name: str
    dependency_type: str  # 'table', 'column', 'procedure'


class SQLMetadataExtractor:
    """Extract metadata from SQL Server using system DMVs."""

    def __init__(self, connection_string: str):
        """
        Initialize extractor.

        Args:
            connection_string: SQL Server connection string
                Example: "DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=FabDB;Trusted_Connection=yes;"
        """
        self.connection_string = connection_string

    def extract_all_metadata(self) -> Dict[str, Any]:
        """
        Extract complete metadata catalog.

        Returns:
            Dictionary with:
                - objects: List[SQLObject]
                - dependencies: List[SQLDependency]
                - statistics: Dict[str, int]
        """
        with pyodbc.connect(self.connection_string) as conn:
            cursor = conn.cursor()

            # Extract objects
            objects = self._extract_objects(cursor)

            # Extract dependencies
            dependencies = self._extract_dependencies(cursor)

            # Statistics
            stats = {
                'total_objects': len(objects),
                'stored_procedures': sum(1 for obj in objects if obj.object_type == 'procedure'),
                'tables': sum(1 for obj in objects if obj.object_type == 'table'),
                'views': sum(1 for obj in objects if obj.object_type == 'view'),
                'dependencies': len(dependencies)
            }

            return {
                'objects': objects,
                'dependencies': dependencies,
                'statistics': stats
            }

    def _extract_objects(self, cursor) -> List[SQLObject]:
        """Extract all database objects (tables, views, procs, functions)."""

        query = """
        SELECT
            obj.object_id,
            obj.name,
            SCHEMA_NAME(obj.schema_id) AS schema_name,
            obj.type_desc,
            mod.definition,
            obj.create_date,
            obj.modify_date
        FROM sys.objects obj
        LEFT JOIN sys.sql_modules mod ON obj.object_id = mod.object_id
        WHERE obj.type IN ('U', 'V', 'P', 'FN', 'IF', 'TF')
            -- U=Table, V=View, P=Procedure, FN=Scalar Function, IF/TF=Table Functions
            AND obj.is_ms_shipped = 0  -- Exclude system objects
        ORDER BY obj.name
        """

        cursor.execute(query)
        rows = cursor.fetchall()

        objects = []
        for row in rows:
            obj_type_map = {
                'USER_TABLE': 'table',
                'VIEW': 'view',
                'SQL_STORED_PROCEDURE': 'procedure',
                'SQL_SCALAR_FUNCTION': 'function',
                'SQL_INLINE_TABLE_VALUED_FUNCTION': 'function',
                'SQL_TABLE_VALUED_FUNCTION': 'function'
            }

            objects.append(SQLObject(
                object_id=row.object_id,
                name=row.name,
                schema_name=row.schema_name,
                object_type=obj_type_map.get(row.type_desc, 'unknown'),
                definition=row.definition,
                created_date=str(row.create_date) if row.create_date else None,
                modified_date=str(row.modify_date) if row.modify_date else None
            ))

        return objects

    def _extract_dependencies(self, cursor) -> List[SQLDependency]:
        """Extract object dependencies using DMVs."""

        query = """
        SELECT DISTINCT
            ref_obj.object_id AS referencing_object_id,
            SCHEMA_NAME(ref_obj.schema_id) + '.' + ref_obj.name AS referencing_object_name,
            referenced_obj.object_id AS referenced_object_id,
            SCHEMA_NAME(referenced_obj.schema_id) + '.' + referenced_obj.name AS referenced_object_name,
            referenced_obj.type_desc AS dependency_type
        FROM sys.sql_expression_dependencies dep
        INNER JOIN sys.objects ref_obj ON dep.referencing_id = ref_obj.object_id
        INNER JOIN sys.objects referenced_obj ON dep.referenced_id = referenced_obj.object_id
        WHERE ref_obj.is_ms_shipped = 0
            AND referenced_obj.is_ms_shipped = 0
        ORDER BY referencing_object_name, referenced_object_name
        """

        cursor.execute(query)
        rows = cursor.fetchall()

        dependencies = []
        for row in rows:
            dependencies.append(SQLDependency(
                referencing_object_id=row.referencing_object_id,
                referencing_object_name=row.referencing_object_name,
                referenced_object_id=row.referenced_object_id,
                referenced_object_name=row.referenced_object_name,
                dependency_type=row.dependency_type
            ))

        return dependencies

    def extract_table_columns(self, table_name: str, schema_name: str = 'dbo') -> List[Dict[str, Any]]:
        """Extract column metadata for a specific table."""

        with pyodbc.connect(self.connection_string) as conn:
            cursor = conn.cursor()

            query = """
            SELECT
                c.name AS column_name,
                t.name AS data_type,
                c.max_length,
                c.precision,
                c.scale,
                c.is_nullable
            FROM sys.columns c
            INNER JOIN sys.types t ON c.user_type_id = t.user_type_id
            INNER JOIN sys.objects o ON c.object_id = o.object_id
            WHERE o.name = ?
                AND SCHEMA_NAME(o.schema_id) = ?
            ORDER BY c.column_id
            """

            cursor.execute(query, (table_name, schema_name))
            rows = cursor.fetchall()

            columns = []
            for row in rows:
                columns.append({
                    'column_name': row.column_name,
                    'data_type': row.data_type,
                    'max_length': row.max_length,
                    'precision': row.precision,
                    'scale': row.scale,
                    'is_nullable': row.is_nullable
                })

            return columns


def save_metadata_to_json(metadata: Dict[str, Any], output_path: Path):
    """Save extracted metadata to JSON file."""
    import json
    from dataclasses import asdict

    # Convert dataclasses to dicts
    serializable = {
        'objects': [asdict(obj) for obj in metadata['objects']],
        'dependencies': [asdict(dep) for dep in metadata['dependencies']],
        'statistics': metadata['statistics']
    }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(serializable, f, indent=2)
```

#### 2. CLI for Metadata Extraction

**File:** `src/atlassemi/pipeline/__main__.py`

**Purpose:** Command-line interface to run metadata extraction.

```python
"""
ATLASsemi Database Pipeline Debugger

Usage:
    python -m atlassemi.pipeline extract-sql --connection-string "..." --output metadata.json
    python -m atlassemi.pipeline extract-powerbi --workspace-id "..." --output lineage.json
"""

import argparse
import sys
from pathlib import Path
from atlassemi.pipeline.sql_metadata_extractor import SQLMetadataExtractor, save_metadata_to_json


def extract_sql_metadata(args):
    """Extract SQL Server metadata."""
    print(f"Extracting metadata from SQL Server...")
    print(f"Connection: {args.connection_string[:50]}...")

    extractor = SQLMetadataExtractor(args.connection_string)
    metadata = extractor.extract_all_metadata()

    print(f"\nExtraction complete:")
    print(f"  Tables: {metadata['statistics']['tables']}")
    print(f"  Stored Procedures: {metadata['statistics']['stored_procedures']}")
    print(f"  Views: {metadata['statistics']['views']}")
    print(f"  Dependencies: {metadata['statistics']['dependencies']}")

    output_path = Path(args.output)
    save_metadata_to_json(metadata, output_path)
    print(f"\nMetadata saved to: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="ATLASsemi Database Pipeline Debugger",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    subparsers = parser.add_subparsers(dest='command', help='Command to execute')

    # SQL metadata extraction
    sql_parser = subparsers.add_parser('extract-sql', help='Extract SQL Server metadata')
    sql_parser.add_argument(
        '--connection-string',
        required=True,
        help='SQL Server connection string'
    )
    sql_parser.add_argument(
        '--output',
        default='pipeline_metadata/sql_metadata.json',
        help='Output JSON file path'
    )
    sql_parser.set_defaults(func=extract_sql_metadata)

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    args.func(args)


if __name__ == '__main__':
    main()
```

### Success Criteria

#### Automated Verification:
- [ ] SQL metadata extraction runs without errors: `python -m atlassemi.pipeline extract-sql --connection-string "..." --output test.json`
- [ ] Output JSON contains expected structure (objects, dependencies, statistics)
- [ ] Unit tests pass: `pytest tests/pipeline/test_sql_metadata_extractor.py`
- [ ] Type checking passes: `mypy src/atlassemi/pipeline/`
- [ ] Code formatted: `black src/atlassemi/pipeline/ && isort src/atlassemi/pipeline/`

#### Manual Verification:
- [ ] Extracted metadata includes all user-created stored procedures (spot check 5-10 procs)
- [ ] Dependencies correctly map stored proc → tables (verify 3-5 dependencies manually)
- [ ] No sensitive data leaked in metadata (review output JSON)
- [ ] Extraction completes in reasonable time (<2 minutes for typical database)

**Implementation Note:** After Phase 1 completion and automated tests pass, verify manually that metadata is accurate before proceeding to Phase 2.

---

## Phase 2: Power BI Lineage Extraction

### Overview

Extract lineage metadata from Power BI using Admin Scanner API to trace reports → datasets → queries → tables.

**Goal:** Map every Power BI report/measure back to source SQL queries and tables.

**Duration:** 5-7 hours

### Changes Required

#### 1. Power BI Scanner Module

**File:** `src/atlassemi/pipeline/powerbi_scanner.py`

**Purpose:** Query Power BI Admin API for lineage metadata.

```python
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
import requests
import json
import time
from pathlib import Path


@dataclass
class PowerBIReport:
    """Power BI report metadata."""
    report_id: str
    report_name: str
    dataset_id: str
    dataset_name: str
    workspace_id: str
    workspace_name: str


@dataclass
class PowerBIDataset:
    """Power BI dataset (semantic model) metadata."""
    dataset_id: str
    dataset_name: str
    workspace_id: str
    tables: List[str]
    expressions: Optional[Dict[str, str]] = None  # DAX expressions
    data_sources: Optional[List[Dict[str, Any]]] = None


@dataclass
class PowerBILineage:
    """Complete lineage from report to data source."""
    report: PowerBIReport
    dataset: PowerBIDataset
    sql_tables: List[str]  # Extracted from dataset queries


class PowerBIScanner:
    """
    Extract lineage from Power BI using Admin Scanner API.

    Requires Power BI admin rights.

    API Reference:
    https://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-get-scan-result
    """

    def __init__(self, tenant_id: str, client_id: str, client_secret: str):
        """
        Initialize scanner with Azure AD credentials.

        Args:
            tenant_id: Azure AD tenant ID
            client_id: Service principal client ID
            client_secret: Service principal client secret
        """
        self.tenant_id = tenant_id
        self.client_id = client_id
        self.client_secret = client_secret
        self.access_token = None

    def authenticate(self):
        """Get access token for Power BI API."""

        url = f"https://login.microsoftonline.com/{self.tenant_id}/oauth2/v2.0/token"

        data = {
            'grant_type': 'client_credentials',
            'client_id': self.client_id,
            'client_secret': self.client_secret,
            'scope': 'https://analysis.windows.net/powerbi/api/.default'
        }

        response = requests.post(url, data=data)
        response.raise_for_status()

        self.access_token = response.json()['access_token']
        print("✓ Authenticated with Power BI API")

    def scan_workspaces(self, workspace_ids: Optional[List[str]] = None) -> str:
        """
        Initiate workspace scan.

        Args:
            workspace_ids: List of workspace IDs to scan (None = all workspaces)

        Returns:
            scan_id: ID to use for retrieving results
        """
        if not self.access_token:
            self.authenticate()

        url = "https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo"

        headers = {
            'Authorization': f'Bearer {self.access_token}',
            'Content-Type': 'application/json'
        }

        params = {
            'lineage': 'True',
            'datasourceDetails': 'True',
            'datasetSchema': 'True',
            'datasetExpressions': 'True'
        }

        body = {}
        if workspace_ids:
            body['workspaces'] = workspace_ids

        response = requests.post(url, headers=headers, params=params, json=body)
        response.raise_for_status()

        scan_id = response.json()['id']
        print(f"✓ Scan initiated: {scan_id}")
        return scan_id

    def get_scan_result(self, scan_id: str, wait_for_completion: bool = True) -> Dict[str, Any]:
        """
        Retrieve scan results.

        Args:
            scan_id: Scan ID from scan_workspaces()
            wait_for_completion: Poll until scan completes

        Returns:
            Complete scan result with lineage metadata
        """
        if not self.access_token:
            self.authenticate()

        url = f"https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/{scan_id}"

        headers = {
            'Authorization': f'Bearer {self.access_token}'
        }

        if wait_for_completion:
            print("Waiting for scan to complete...", end='')
            while True:
                response = requests.get(url, headers=headers)
                response.raise_for_status()

                result = response.json()
                status = result.get('status')

                if status == 'Succeeded':
                    print(" Done!")
                    return result
                elif status == 'Failed':
                    raise Exception(f"Scan failed: {result}")

                print(".", end='', flush=True)
                time.sleep(5)  # Poll every 5 seconds
        else:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            return response.json()

    def extract_lineage(self, scan_result: Dict[str, Any]) -> List[PowerBILineage]:
        """
        Parse scan result into lineage objects.

        Args:
            scan_result: Result from get_scan_result()

        Returns:
            List of PowerBILineage objects
        """
        lineages = []

        workspaces = scan_result.get('workspaces', [])

        for workspace in workspaces:
            workspace_id = workspace['id']
            workspace_name = workspace['name']

            # Build dataset lookup
            datasets = {}
            for dataset in workspace.get('datasets', []):
                datasets[dataset['id']] = PowerBIDataset(
                    dataset_id=dataset['id'],
                    dataset_name=dataset['name'],
                    workspace_id=workspace_id,
                    tables=[table['name'] for table in dataset.get('tables', [])],
                    expressions=dataset.get('expressions'),
                    data_sources=dataset.get('datasources')
                )

            # Map reports to datasets
            for report in workspace.get('reports', []):
                dataset_id = report.get('datasetId')
                if not dataset_id or dataset_id not in datasets:
                    continue

                dataset = datasets[dataset_id]

                # Extract SQL tables from data sources
                sql_tables = []
                if dataset.data_sources:
                    for ds in dataset.data_sources:
                        connection_details = ds.get('connectionDetails', {})
                        if 'path' in connection_details:
                            # Parse table names from connection string
                            # Example: "Server=localhost;Database=FabDB"
                            # Tables are in dataset.tables
                            sql_tables.extend(dataset.tables)

                report_obj = PowerBIReport(
                    report_id=report['id'],
                    report_name=report['name'],
                    dataset_id=dataset_id,
                    dataset_name=dataset.dataset_name,
                    workspace_id=workspace_id,
                    workspace_name=workspace_name
                )

                lineages.append(PowerBILineage(
                    report=report_obj,
                    dataset=dataset,
                    sql_tables=list(set(sql_tables))  # Deduplicate
                ))

        return lineages


def save_lineage_to_json(lineages: List[PowerBILineage], output_path: Path):
    """Save lineage to JSON file."""
    from dataclasses import asdict

    serializable = [asdict(lineage) for lineage in lineages]

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(serializable, f, indent=2)
```

#### 2. Update CLI for Power BI Extraction

**File:** `src/atlassemi/pipeline/__main__.py` (add to existing)

```python
from atlassemi.pipeline.powerbi_scanner import PowerBIScanner, save_lineage_to_json

def extract_powerbi_lineage(args):
    """Extract Power BI lineage metadata."""
    print(f"Extracting lineage from Power BI...")

    scanner = PowerBIScanner(
        tenant_id=args.tenant_id,
        client_id=args.client_id,
        client_secret=args.client_secret
    )

    # Initiate scan
    scan_id = scanner.scan_workspaces(workspace_ids=args.workspace_ids)

    # Wait for results
    scan_result = scanner.get_scan_result(scan_id, wait_for_completion=True)

    # Extract lineage
    lineages = scanner.extract_lineage(scan_result)

    print(f"\nExtraction complete:")
    print(f"  Reports: {len(lineages)}")
    print(f"  Unique datasets: {len(set(l.dataset.dataset_id for l in lineages))}")

    output_path = Path(args.output)
    save_lineage_to_json(lineages, output_path)
    print(f"\nLineage saved to: {output_path}")


# Add to main() subparsers
powerbi_parser = subparsers.add_parser('extract-powerbi', help='Extract Power BI lineage')
powerbi_parser.add_argument('--tenant-id', required=True, help='Azure AD tenant ID')
powerbi_parser.add_argument('--client-id', required=True, help='Service principal client ID')
powerbi_parser.add_argument('--client-secret', required=True, help='Service principal secret')
powerbi_parser.add_argument('--workspace-ids', nargs='*', help='Workspace IDs to scan (optional)')
powerbi_parser.add_argument('--output', default='pipeline_metadata/powerbi_lineage.json', help='Output JSON file')
powerbi_parser.set_defaults(func=extract_powerbi_lineage)
```

### Success Criteria

#### Automated Verification:
- [ ] Power BI authentication succeeds: `python -m atlassemi.pipeline extract-powerbi --tenant-id "..." --client-id "..." --client-secret "..." --output test.json`
- [ ] Scan completes and returns results
- [ ] Unit tests pass: `pytest tests/pipeline/test_powerbi_scanner.py`
- [ ] Type checking passes: `mypy src/atlassemi/pipeline/`

#### Manual Verification:
- [ ] Extracted lineage includes known reports (spot check 3-5 reports)
- [ ] Report → dataset → SQL table mapping is correct (verify 3-5 lineages manually)
- [ ] DAX expressions captured for datasets with custom measures
- [ ] No authentication errors or permission issues

**Implementation Note:** After Phase 2 completion, verify that lineage correctly maps at least 3 production reports to their SQL sources before proceeding.

---

## Phase 3: Code Embedding & Semantic Search

### Overview

Embed SQL stored procedure code using vector embeddings to enable semantic search ("find procs that calculate Cpk").

**Goal:** Enable natural language queries over SQL code.

**Duration:** 6-8 hours

### Changes Required

#### 1. SQL Code Chunker

**File:** `src/atlassemi/pipeline/sql_code_chunker.py`

**Purpose:** Split SQL stored procedures into meaningful chunks for embedding.

```python
from dataclasses import dataclass
from typing import List, Dict, Any
import re


@dataclass
class SQLCodeChunk:
    """A chunk of SQL code with metadata."""
    procedure_name: str
    schema_name: str
    chunk_text: str
    chunk_index: int
    line_start: int
    line_end: int
    chunk_type: str  # 'header', 'logic', 'query', 'comment'


class SQLCodeChunker:
    """
    Chunk SQL stored procedures for embedding.

    Strategy:
    - Preserve logical units (complete SELECT/UPDATE/DELETE statements)
    - Keep chunk size ~500-800 tokens (roughly 300-500 words)
    - Add 10-20% overlap for context
    - Tag chunk types (header, logic, query, comment)
    """

    def __init__(self, target_chunk_size: int = 600, overlap_tokens: int = 100):
        self.target_chunk_size = target_chunk_size
        self.overlap_tokens = overlap_tokens

    def chunk_procedure(self, procedure_name: str, schema_name: str, sql_code: str) -> List[SQLCodeChunk]:
        """
        Chunk a stored procedure into semantic units.

        Args:
            procedure_name: Procedure name
            schema_name: Schema name
            sql_code: Full SQL procedure code

        Returns:
            List of SQLCodeChunk objects
        """
        if not sql_code or not sql_code.strip():
            return []

        # Split into lines for line number tracking
        lines = sql_code.split('\n')

        # Strategy: Split on major SQL keywords while preserving context
        chunks = []
        current_chunk = []
        current_line_start = 1
        chunk_index = 0

        # Keywords that indicate chunk boundaries
        major_keywords = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'MERGE', 'WITH', 'CREATE', 'ALTER']

        for i, line in enumerate(lines, start=1):
            line_upper = line.strip().upper()

            # Check if this line starts a new major statement
            is_boundary = any(line_upper.startswith(kw) for kw in major_keywords)

            # Estimate current chunk size (rough token count: words * 1.3)
            current_text = '\n'.join(current_chunk)
            estimated_tokens = len(current_text.split()) * 1.3

            if is_boundary and estimated_tokens > self.target_chunk_size:
                # Flush current chunk
                if current_chunk:
                    chunk_type = self._classify_chunk('\n'.join(current_chunk))
                    chunks.append(SQLCodeChunk(
                        procedure_name=procedure_name,
                        schema_name=schema_name,
                        chunk_text='\n'.join(current_chunk),
                        chunk_index=chunk_index,
                        line_start=current_line_start,
                        line_end=i - 1,
                        chunk_type=chunk_type
                    ))
                    chunk_index += 1
                    current_chunk = []
                    current_line_start = i

            current_chunk.append(line)

        # Flush remaining chunk
        if current_chunk:
            chunk_type = self._classify_chunk('\n'.join(current_chunk))
            chunks.append(SQLCodeChunk(
                procedure_name=procedure_name,
                schema_name=schema_name,
                chunk_text='\n'.join(current_chunk),
                chunk_index=chunk_index,
                line_start=current_line_start,
                line_end=len(lines),
                chunk_type=chunk_type
            ))

        return chunks

    def _classify_chunk(self, chunk_text: str) -> str:
        """Classify chunk type based on content."""
        upper_text = chunk_text.upper()

        if 'CREATE PROCEDURE' in upper_text or 'ALTER PROCEDURE' in upper_text:
            return 'header'
        elif 'SELECT' in upper_text:
            return 'query'
        elif chunk_text.strip().startswith('--') or chunk_text.strip().startswith('/*'):
            return 'comment'
        else:
            return 'logic'
```

#### 2. Code Embedder

**File:** `src/atlassemi/pipeline/sql_code_embedder.py`

**Purpose:** Generate embeddings for SQL code chunks (local model for Tier 2 security).

```python
from dataclasses import dataclass
from typing import List, Dict, Any
from pathlib import Path
import numpy as np

# Try SQL Server 2025 native vectors first, fall back to sentence-transformers
try:
    import pyodbc
    SQL_SERVER_2025_AVAILABLE = True
except ImportError:
    SQL_SERVER_2025_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False


@dataclass
class CodeEmbedding:
    """Embedded SQL code chunk."""
    procedure_name: str
    schema_name: str
    chunk_index: int
    chunk_text: str
    embedding: List[float]
    metadata: Dict[str, Any]


class SQLCodeEmbedder:
    """
    Generate embeddings for SQL code.

    Priority:
    1. SQL Server 2025 native vectors (if available)
    2. sentence-transformers (local, Tier 2 safe)
    3. Fallback error (no OpenAI for confidential code)
    """

    def __init__(self, method: str = 'auto', model_name: str = 'all-MiniLM-L6-v2'):
        """
        Initialize embedder.

        Args:
            method: 'sql_server_2025', 'sentence_transformers', or 'auto'
            model_name: Model name for sentence-transformers
        """
        self.method = method
        self.model_name = model_name
        self.model = None

        if method == 'auto':
            if SQL_SERVER_2025_AVAILABLE:
                self.method = 'sql_server_2025'
                print("Using SQL Server 2025 native vectors")
            elif SENTENCE_TRANSFORMERS_AVAILABLE:
                self.method = 'sentence_transformers'
                print(f"Using sentence-transformers: {model_name}")
            else:
                raise Exception("No embedding method available. Install sentence-transformers or use SQL Server 2025.")

        if self.method == 'sentence_transformers':
            self.model = SentenceTransformer(model_name)

    def embed_chunks(self, chunks: List['SQLCodeChunk']) -> List[CodeEmbedding]:
        """
        Embed SQL code chunks.

        Args:
            chunks: List of SQLCodeChunk objects

        Returns:
            List of CodeEmbedding objects
        """
        if self.method == 'sentence_transformers':
            return self._embed_with_sentence_transformers(chunks)
        elif self.method == 'sql_server_2025':
            return self._embed_with_sql_server(chunks)
        else:
            raise NotImplementedError(f"Method {self.method} not implemented")

    def _embed_with_sentence_transformers(self, chunks: List['SQLCodeChunk']) -> List[CodeEmbedding]:
        """Embed using sentence-transformers (local model)."""

        texts = [chunk.chunk_text for chunk in chunks]
        embeddings = self.model.encode(texts, show_progress_bar=True)

        results = []
        for i, chunk in enumerate(chunks):
            results.append(CodeEmbedding(
                procedure_name=chunk.procedure_name,
                schema_name=chunk.schema_name,
                chunk_index=chunk.chunk_index,
                chunk_text=chunk.chunk_text,
                embedding=embeddings[i].tolist(),
                metadata={
                    'line_start': chunk.line_start,
                    'line_end': chunk.line_end,
                    'chunk_type': chunk.chunk_type
                }
            ))

        return results

    def _embed_with_sql_server(self, chunks: List['SQLCodeChunk']) -> List[CodeEmbedding]:
        """Embed using SQL Server 2025 AI_GENERATE_EMBEDDINGS."""
        # Implementation would use SQL Server 2025 AI functions
        # Left as exercise - requires SQL Server 2025 connection
        raise NotImplementedError("SQL Server 2025 embeddings not yet implemented")
```

#### 3. ChromaDB Code Index

**File:** `src/atlassemi/pipeline/code_index.py`

**Purpose:** Store and query SQL code embeddings in ChromaDB.

```python
from typing import List, Dict, Any, Optional
import chromadb
from chromadb.config import Settings
from pathlib import Path


class SQLCodeIndex:
    """
    ChromaDB-based index for SQL code semantic search.

    Security: Local ChromaDB only (no cloud) for Tier 2 confidential code.
    """

    def __init__(self, persist_directory: str = "pipeline_metadata/chromadb"):
        """
        Initialize code index.

        Args:
            persist_directory: Directory for ChromaDB persistence
        """
        self.persist_directory = Path(persist_directory)
        self.persist_directory.mkdir(parents=True, exist_ok=True)

        self.client = chromadb.Client(Settings(
            persist_directory=str(self.persist_directory),
            anonymized_telemetry=False
        ))

        # Create or get collection
        self.collection = self.client.get_or_create_collection(
            name="sql_code_chunks",
            metadata={"description": "SQL stored procedure code chunks for semantic search"}
        )

    def index_code_embeddings(self, embeddings: List['CodeEmbedding']):
        """
        Add code embeddings to index.

        Args:
            embeddings: List of CodeEmbedding objects
        """
        if not embeddings:
            return

        # Prepare data for ChromaDB
        ids = [f"{emb.schema_name}.{emb.procedure_name}:{emb.chunk_index}" for emb in embeddings]
        documents = [emb.chunk_text for emb in embeddings]
        embeddings_vectors = [emb.embedding for emb in embeddings]
        metadatas = [
            {
                'procedure_name': emb.procedure_name,
                'schema_name': emb.schema_name,
                'chunk_index': emb.chunk_index,
                **emb.metadata
            }
            for emb in embeddings
        ]

        # Add to collection (upsert to handle updates)
        self.collection.add(
            ids=ids,
            documents=documents,
            embeddings=embeddings_vectors,
            metadatas=metadatas
        )

        print(f"✓ Indexed {len(embeddings)} code chunks")

    def semantic_search(self, query: str, top_k: int = 10,
                        filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Semantic search over SQL code.

        Args:
            query: Natural language query (e.g., "find procs that calculate Cpk")
            top_k: Number of results to return
            filters: Optional metadata filters

        Returns:
            List of results with code chunks and metadata
        """
        # Note: ChromaDB will use the same embedding model to embed the query
        # For now, this requires the embedding model to be available
        # In production, you'd embed the query separately

        results = self.collection.query(
            query_texts=[query],
            n_results=top_k,
            where=filters
        )

        # Format results
        formatted_results = []
        for i in range(len(results['ids'][0])):
            formatted_results.append({
                'procedure_name': results['metadatas'][0][i]['procedure_name'],
                'schema_name': results['metadatas'][0][i]['schema_name'],
                'chunk_index': results['metadatas'][0][i]['chunk_index'],
                'line_start': results['metadatas'][0][i]['line_start'],
                'line_end': results['metadatas'][0][i]['line_end'],
                'chunk_type': results['metadatas'][0][i]['chunk_type'],
                'code': results['documents'][0][i],
                'distance': results['distances'][0][i] if 'distances' in results else None
            })

        return formatted_results
```

### Success Criteria

#### Automated Verification:
- [ ] Code chunking produces reasonable chunks: `pytest tests/pipeline/test_sql_code_chunker.py`
- [ ] Embeddings generate successfully with sentence-transformers
- [ ] ChromaDB index creation and insertion succeeds
- [ ] Semantic search returns results: Test query "calculate yield" returns relevant procs
- [ ] Type checking passes: `mypy src/atlassemi/pipeline/`

#### Manual Verification:
- [ ] Chunk boundaries respect SQL logical units (complete SELECT statements, etc.)
- [ ] Semantic search relevance: Query "Cpk calculation" returns procs that calculate Cpk
- [ ] Search performance: Query returns results in <2 seconds
- [ ] No false positives: Verify top 5 results for 3 test queries are relevant

**Implementation Note:** After Phase 3, test semantic search with at least 5 different queries to verify relevance before proceeding.

---

## Phase 4: Dependency Graph Construction

### Overview

Build a queryable dependency graph combining SQL metadata and Power BI lineage.

**Goal:** Answer questions like "Which reports break if I change table X?" or "Show me the full lineage for Report Y".

**Duration:** 4-6 hours

### Changes Required

#### 1. Dependency Graph Builder

**File:** `src/atlassemi/pipeline/dependency_graph.py`

**Purpose:** Combine SQL and Power BI metadata into unified dependency graph.

```python
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Set
from pathlib import Path
import json


@dataclass
class GraphNode:
    """A node in the dependency graph."""
    node_id: str
    node_type: str  # 'report', 'dataset', 'procedure', 'table', 'view'
    name: str
    metadata: Dict[str, Any]


@dataclass
class GraphEdge:
    """A directed edge in the dependency graph."""
    source_id: str  # References node_id
    target_id: str  # References node_id
    edge_type: str  # 'uses', 'contains', 'queries'


class DependencyGraph:
    """
    Unified dependency graph for SQL Server + Power BI lineage.

    Graph structure:
    - Reports → Datasets (uses)
    - Datasets → SQL Tables (queries)
    - Stored Procedures → Tables (references)
    - Views → Tables (references)

    Enables queries:
    - Downstream impact: "What reports use table X?"
    - Upstream lineage: "Where does Report Y get its data?"
    - Procedure dependencies: "What tables does sp_Calculate_Yield use?"
    """

    def __init__(self):
        self.nodes: Dict[str, GraphNode] = {}
        self.edges: List[GraphEdge] = []

    def add_node(self, node: GraphNode):
        """Add or update a node in the graph."""
        self.nodes[node.node_id] = node

    def add_edge(self, edge: GraphEdge):
        """Add a directed edge to the graph."""
        self.edges.append(edge)

    def build_from_metadata(self,
                           sql_metadata: Dict[str, Any],
                           powerbi_lineage: List[Dict[str, Any]]):
        """
        Build graph from SQL metadata and Power BI lineage.

        Args:
            sql_metadata: Output from SQLMetadataExtractor
            powerbi_lineage: Output from PowerBIScanner
        """
        # Add SQL nodes
        for obj in sql_metadata['objects']:
            node_id = f"sql.{obj['schema_name']}.{obj['name']}"
            self.add_node(GraphNode(
                node_id=node_id,
                node_type=obj['object_type'],
                name=f"{obj['schema_name']}.{obj['name']}",
                metadata={
                    'schema': obj['schema_name'],
                    'object_id': obj['object_id'],
                    'definition': obj.get('definition'),
                    'created_date': obj.get('created_date'),
                    'modified_date': obj.get('modified_date')
                }
            ))

        # Add SQL dependencies
        for dep in sql_metadata['dependencies']:
            source_id = f"sql.{dep['referencing_object_name']}"
            target_id = f"sql.{dep['referenced_object_name']}"

            self.add_edge(GraphEdge(
                source_id=source_id,
                target_id=target_id,
                edge_type='references'
            ))

        # Add Power BI nodes and edges
        for lineage in powerbi_lineage:
            # Add report node
            report = lineage['report']
            report_id = f"powerbi.report.{report['report_id']}"
            self.add_node(GraphNode(
                node_id=report_id,
                node_type='report',
                name=report['report_name'],
                metadata={
                    'workspace_id': report['workspace_id'],
                    'workspace_name': report['workspace_name'],
                    'dataset_id': report['dataset_id']
                }
            ))

            # Add dataset node
            dataset = lineage['dataset']
            dataset_id = f"powerbi.dataset.{dataset['dataset_id']}"
            self.add_node(GraphNode(
                node_id=dataset_id,
                node_type='dataset',
                name=dataset['dataset_name'],
                metadata={
                    'workspace_id': dataset['workspace_id'],
                    'tables': dataset['tables']
                }
            ))

            # Edge: Report → Dataset
            self.add_edge(GraphEdge(
                source_id=report_id,
                target_id=dataset_id,
                edge_type='uses'
            ))

            # Edges: Dataset → SQL Tables
            for table_name in lineage['sql_tables']:
                # Assume dbo schema if not specified
                sql_table_id = f"sql.dbo.{table_name}" if '.' not in table_name else f"sql.{table_name}"

                self.add_edge(GraphEdge(
                    source_id=dataset_id,
                    target_id=sql_table_id,
                    edge_type='queries'
                ))

    def find_downstream_impact(self, node_id: str, max_depth: int = 10) -> List[GraphNode]:
        """
        Find all downstream nodes affected by changes to this node.

        Example: "Which reports use table tblChamberMetrics?"

        Args:
            node_id: Starting node (e.g., "sql.dbo.tblChamberMetrics")
            max_depth: Maximum traversal depth

        Returns:
            List of affected nodes (reports, datasets, procedures)
        """
        visited: Set[str] = set()
        affected: List[GraphNode] = []

        def traverse(current_id: str, depth: int):
            if depth > max_depth or current_id in visited:
                return

            visited.add(current_id)

            # Find all edges where current node is the target (something depends on it)
            for edge in self.edges:
                if edge.target_id == current_id:
                    source_node = self.nodes.get(edge.source_id)
                    if source_node:
                        affected.append(source_node)
                        traverse(edge.source_id, depth + 1)

        traverse(node_id, 0)
        return affected

    def find_upstream_lineage(self, node_id: str, max_depth: int = 10) -> List[GraphNode]:
        """
        Find all upstream nodes that contribute to this node.

        Example: "Where does 'Yield Dashboard' get its data?"

        Args:
            node_id: Starting node (e.g., "powerbi.report.abc123")
            max_depth: Maximum traversal depth

        Returns:
            List of source nodes (datasets, tables, procedures)
        """
        visited: Set[str] = set()
        sources: List[GraphNode] = []

        def traverse(current_id: str, depth: int):
            if depth > max_depth or current_id in visited:
                return

            visited.add(current_id)

            # Find all edges where current node is the source (it depends on something)
            for edge in self.edges:
                if edge.source_id == current_id:
                    target_node = self.nodes.get(edge.target_id)
                    if target_node:
                        sources.append(target_node)
                        traverse(edge.target_id, depth + 1)

        traverse(node_id, 0)
        return sources

    def export_to_json(self, output_path: Path):
        """Export graph to JSON for inspection."""
        from dataclasses import asdict

        graph_data = {
            'nodes': [asdict(node) for node in self.nodes.values()],
            'edges': [asdict(edge) for edge in self.edges]
        }

        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(graph_data, f, indent=2)

        print(f"✓ Exported graph: {len(self.nodes)} nodes, {len(self.edges)} edges")
```

#### 2. Query Interface

**File:** `src/atlassemi/pipeline/query_interface.py`

**Purpose:** High-level query API for common lineage questions.

```python
from typing import List, Dict, Any, Optional
from atlassemi.pipeline.dependency_graph import DependencyGraph, GraphNode
from atlassemi.pipeline.code_index import SQLCodeIndex


class PipelineQueryEngine:
    """
    High-level query interface for database pipeline debugging.

    Combines:
    - Dependency graph (structural relationships)
    - Code index (semantic search)
    """

    def __init__(self, graph: DependencyGraph, code_index: SQLCodeIndex):
        self.graph = graph
        self.code_index = code_index

    def which_reports_use_table(self, table_name: str, schema: str = 'dbo') -> List[Dict[str, Any]]:
        """
        Find all Power BI reports that depend on a SQL table.

        Args:
            table_name: SQL table name
            schema: Schema name (default 'dbo')

        Returns:
            List of reports with lineage paths
        """
        node_id = f"sql.{schema}.{table_name}"

        if node_id not in self.graph.nodes:
            return []

        # Find downstream impact
        affected = self.graph.find_downstream_impact(node_id)

        # Filter to reports only
        reports = [node for node in affected if node.node_type == 'report']

        return [
            {
                'report_name': report.name,
                'report_id': report.node_id,
                'workspace': report.metadata.get('workspace_name'),
                'dataset_id': report.metadata.get('dataset_id')
            }
            for report in reports
        ]

    def where_does_report_get_data(self, report_name: str) -> List[Dict[str, Any]]:
        """
        Trace a Power BI report back to source SQL tables.

        Args:
            report_name: Power BI report name

        Returns:
            Lineage path from report to tables
        """
        # Find report node
        report_node = None
        for node in self.graph.nodes.values():
            if node.node_type == 'report' and report_name.lower() in node.name.lower():
                report_node = node
                break

        if not report_node:
            return []

        # Find upstream lineage
        sources = self.graph.find_upstream_lineage(report_node.node_id)

        # Filter to tables
        tables = [node for node in sources if node.node_type == 'table']

        return [
            {
                'table_name': table.name,
                'table_id': table.node_id,
                'schema': table.metadata.get('schema')
            }
            for table in tables
        ]

    def find_procedures_for_query(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Semantic search: Find stored procedures matching natural language query.

        Args:
            query: Natural language query (e.g., "calculate Cpk for chambers")
            top_k: Number of results

        Returns:
            Ranked stored procedures with code snippets
        """
        results = self.code_index.semantic_search(query, top_k=top_k)

        # Group by procedure (multiple chunks per proc)
        procedures = {}
        for result in results:
            proc_name = f"{result['schema_name']}.{result['procedure_name']}"
            if proc_name not in procedures:
                procedures[proc_name] = {
                    'procedure_name': proc_name,
                    'chunks': []
                }
            procedures[proc_name]['chunks'].append({
                'code': result['code'],
                'line_start': result['line_start'],
                'line_end': result['line_end'],
                'relevance_score': 1.0 - result['distance'] if result['distance'] else None
            })

        return list(procedures.values())
```

### Success Criteria

#### Automated Verification:
- [ ] Graph builds successfully from SQL + Power BI metadata
- [ ] Downstream impact query returns correct results: `pytest tests/pipeline/test_dependency_graph.py`
- [ ] Upstream lineage query returns correct results
- [ ] Query interface methods execute without errors
- [ ] Type checking passes: `mypy src/atlassemi/pipeline/`

#### Manual Verification:
- [ ] "Which reports use table X?" returns accurate results (verify 3 tables)
- [ ] "Where does Report Y get data?" traces back to correct SQL tables (verify 3 reports)
- [ ] Graph export visualizes dependencies correctly (inspect JSON)
- [ ] Query performance: Lineage queries complete in <3 seconds

**Implementation Note:** After Phase 4, validate graph correctness by manually tracing 3 report lineages and comparing to system output.

---

## Phase 5: Analysis Agent Integration

### Overview

Integrate pipeline debugger with ATLASsemi's Analysis Agent using new `ProblemMode.DATA_PIPELINE`.

**Goal:** Enable automated troubleshooting of data pipeline issues using 8D methodology.

**Duration:** 5-7 hours

### Changes Required

#### 1. Add DATA_PIPELINE Mode

**File:** `src/atlassemi/agents/base.py`

```python
class ProblemMode(str, Enum):
    """Problem classification modes."""
    EXCURSION = "excursion"
    IMPROVEMENT = "improvement"
    OPERATIONS = "operations"
    DATA_PIPELINE = "data_pipeline"  # NEW
```

#### 2. Pipeline Context Builder

**File:** `src/atlassemi/pipeline/pipeline_context.py`

**Purpose:** Build rich context for Analysis Agent from pipeline metadata.

```python
from typing import Dict, Any, List
from atlassemi.pipeline.query_interface import PipelineQueryEngine


class PipelineContextBuilder:
    """
    Build context for Analysis Agent from pipeline debugging queries.

    Given a data issue description, automatically:
    - Identify mentioned reports/tables
    - Trace lineage
    - Find relevant stored procedures
    - Format as context for 8D analysis
    """

    def __init__(self, query_engine: PipelineQueryEngine):
        self.query_engine = query_engine

    def build_context(self, problem_description: str) -> Dict[str, Any]:
        """
        Extract pipeline context from problem description.

        Args:
            problem_description: User's description of data issue
                Example: "Yield Dashboard shows wrong Chamber B Cpk values"

        Returns:
            Context dict with lineage and relevant code
        """
        context = {
            'lineage': [],
            'relevant_procedures': [],
            'affected_reports': [],
            'recommendations': []
        }

        # Extract entity names (simple keyword matching for MVP)
        # TODO: Use NER or LLM extraction for robustness

        # Look for Power BI report mentions
        if 'dashboard' in problem_description.lower():
            # Extract report name (simplified)
            words = problem_description.split()
            for i, word in enumerate(words):
                if 'dashboard' in word.lower() and i > 0:
                    report_name = words[i-1]

                    # Trace lineage
                    lineage = self.query_engine.where_does_report_get_data(report_name)
                    if lineage:
                        context['lineage'] = lineage

        # Look for table mentions
        if 'table' in problem_description.lower() or 'tbl' in problem_description.lower():
            # Extract table name (simplified)
            import re
            table_pattern = r'\btbl\w+'
            tables = re.findall(table_pattern, problem_description, re.IGNORECASE)

            for table in tables:
                affected = self.query_engine.which_reports_use_table(table)
                context['affected_reports'].extend(affected)

        # Semantic search for relevant procedures
        # Extract key terms for search
        search_query = problem_description.replace('shows wrong', '').replace('incorrect', '')
        procedures = self.query_engine.find_procedures_for_query(search_query, top_k=3)
        context['relevant_procedures'] = procedures

        return context
```

#### 3. Update Analysis Agent Prompt

**File:** `src/atlassemi/agents/analysis_agent.py`

Update the prompt generation to include pipeline context when mode is DATA_PIPELINE.

```python
def generate_prompt(self, agent_input: AgentInput) -> str:
    """Generate Analysis Agent prompt with pipeline context."""

    # ... existing code ...

    # NEW: Add pipeline context for DATA_PIPELINE mode
    if agent_input.mode == ProblemMode.DATA_PIPELINE:
        pipeline_context = agent_input.context.get('pipeline_context', {})

        if pipeline_context:
            prompt += "\n## Pipeline Context\n\n"

            # Lineage information
            if pipeline_context.get('lineage'):
                prompt += "**Data Lineage:**\n"
                for table in pipeline_context['lineage']:
                    prompt += f"  - Source Table: {table['table_name']} (Schema: {table['schema']})\n"
                prompt += "\n"

            # Relevant stored procedures
            if pipeline_context.get('relevant_procedures'):
                prompt += "**Relevant Stored Procedures:**\n"
                for proc in pipeline_context['relevant_procedures']:
                    prompt += f"  - {proc['procedure_name']}\n"
                    for chunk in proc['chunks'][:2]:  # Top 2 chunks
                        prompt += f"    Lines {chunk['line_start']}-{chunk['line_end']}:\n"
                        prompt += f"    ```sql\n{chunk['code']}\n    ```\n"
                prompt += "\n"

            # Affected reports
            if pipeline_context.get('affected_reports'):
                prompt += "**Affected Reports:**\n"
                for report in pipeline_context['affected_reports']:
                    prompt += f"  - {report['report_name']} (Workspace: {report['workspace']})\n"
                prompt += "\n"

    # ... rest of prompt ...

    return prompt
```

#### 4. Pipeline-Specific Orchestrator

**File:** `src/atlassemi/pipeline/pipeline_orchestrator.py`

**Purpose:** Orchestrate DATA_PIPELINE workflow with pipeline context injection.

```python
from atlassemi.agents import NarrativeAgent, ClarificationAgent, AnalysisAgent
from atlassemi.agents.base import AgentInput, AgentOutput, ProblemMode, SecurityTier
from atlassemi.pipeline.query_interface import PipelineQueryEngine
from atlassemi.pipeline.pipeline_context import PipelineContextBuilder
from typing import Dict, Any


class PipelineOrchestrator:
    """
    Orchestrate DATA_PIPELINE workflow.

    Workflow:
    1. Narrative extraction (Phase 0)
    2. Build pipeline context (NEW)
    3. Clarification with context (Phase 1)
    4. Analysis with pipeline context (Phase 2)
    5. Prevention (Phase 3)
    """

    def __init__(self, model_router, query_engine: PipelineQueryEngine):
        self.model_router = model_router
        self.query_engine = query_engine
        self.context_builder = PipelineContextBuilder(query_engine)

        # Initialize agents
        self.narrative_agent = NarrativeAgent(model_router)
        self.clarification_agent = ClarificationAgent(model_router)
        self.analysis_agent = AnalysisAgent(model_router)

    def run_pipeline_debugging(self,
                               narrative: str,
                               security_tier: SecurityTier) -> Dict[str, Any]:
        """
        Run full pipeline debugging workflow.

        Args:
            narrative: User's description of data issue
            security_tier: Security tier (likely CONFIDENTIAL_FAB for real data)

        Returns:
            Complete analysis with 8D mapping and code references
        """
        # Phase 0: Extract observations
        phase0_input = AgentInput(
            mode=ProblemMode.DATA_PIPELINE,
            security_tier=security_tier,
            context={'narrative': narrative}
        )

        phase0_output = self.narrative_agent.execute(phase0_input)

        # NEW: Build pipeline context
        pipeline_context = self.context_builder.build_context(narrative)

        # Phase 1: Clarification
        phase1_input = AgentInput(
            mode=ProblemMode.DATA_PIPELINE,
            security_tier=security_tier,
            context={
                'narrative_analysis': phase0_output.content,
                'pipeline_context': pipeline_context
            }
        )

        phase1_output = self.clarification_agent.execute(phase1_input)

        # TODO: Collect user answers (simplified for MVP - skip Q&A)

        # Phase 2: Analysis with pipeline context
        phase2_input = AgentInput(
            mode=ProblemMode.DATA_PIPELINE,
            security_tier=security_tier,
            context={
                'narrative_analysis': phase0_output.content,
                'clarification_questions': phase1_output.content,
                'pipeline_context': pipeline_context  # Inject here
            }
        )

        phase2_output = self.analysis_agent.execute(phase2_input)

        return {
            'narrative_analysis': phase0_output.content,
            'pipeline_context': pipeline_context,
            'clarification': phase1_output.content,
            'analysis': phase2_output.content
        }
```

### Success Criteria

#### Automated Verification:
- [ ] ProblemMode.DATA_PIPELINE enum added
- [ ] Pipeline context builder extracts entities from problem description
- [ ] Analysis Agent generates prompt with pipeline context
- [ ] Type checking passes: `mypy src/atlassemi/`
- [ ] Unit tests pass: `pytest tests/pipeline/`

#### Manual Verification:
- [ ] End-to-end test: Input "Yield Dashboard shows wrong Cpk" → Analysis includes lineage + code
- [ ] Context quality: Verify 3 test cases have relevant procedures and tables in context
- [ ] 8D mapping: Analysis correctly maps data issues to D1-D3 phases
- [ ] No regression: Existing modes (EXCURSION, IMPROVEMENT, OPERATIONS) still work

**Implementation Note:** After Phase 5, run at least 3 end-to-end tests with realistic data issue descriptions.

---

## Phase 6: Testing with Production Tickets

### Overview

Validate v1.2 with real production data tickets to verify business value.

**Goal:** Demonstrate 5-10x faster resolution on actual tickets.

**Duration:** 4-6 hours

### Changes Required

#### 1. Test Harness

**File:** `tests/pipeline/test_production_tickets.py`

**Purpose:** Structured tests using real production tickets.

```python
import pytest
from atlassemi.pipeline.pipeline_orchestrator import PipelineOrchestrator
from atlassemi.agents.base import SecurityTier


@pytest.fixture
def production_tickets():
    """
    Real production tickets (sanitized).

    Format:
    {
        'ticket_id': str,
        'description': str,
        'expected_root_cause': str,  # For validation
        'expected_tables': List[str],
        'expected_procedures': List[str]
    }
    """
    return [
        {
            'ticket_id': 'DATA-001',
            'description': 'Yield Dashboard Chamber B Cpk values are 0.5 lower than manual calculation',
            'expected_root_cause': 'sp_Calculate_Chamber_Metrics JOIN mismatch',
            'expected_tables': ['tblChamberMetrics', 'tblWaferYield'],
            'expected_procedures': ['sp_Calculate_Chamber_Metrics']
        },
        {
            'ticket_id': 'DATA-002',
            'description': 'Defect Count Report shows duplicate records for Lot ABC123',
            'expected_root_cause': 'sp_Aggregate_Defects missing DISTINCT clause',
            'expected_tables': ['tblDefects'],
            'expected_procedures': ['sp_Aggregate_Defects']
        },
        {
            'ticket_id': 'DATA-003',
            'description': 'WIP Dashboard shows incorrect tool availability percentage',
            'expected_root_cause': 'sp_Tool_Availability calculates uptime incorrectly',
            'expected_tables': ['tblToolStatus', 'tblDowntime'],
            'expected_procedures': ['sp_Tool_Availability', 'sp_Calculate_Uptime']
        }
    ]


@pytest.mark.integration
def test_production_ticket_resolution(production_tickets, pipeline_orchestrator):
    """
    Test v1.2 resolves production tickets correctly.

    Success criteria:
    - Identifies correct tables
    - Finds relevant stored procedures
    - Analysis includes root cause mention
    """
    for ticket in production_tickets:
        print(f"\n=== Testing Ticket: {ticket['ticket_id']} ===")
        print(f"Description: {ticket['description']}")

        # Run pipeline debugging
        result = pipeline_orchestrator.run_pipeline_debugging(
            narrative=ticket['description'],
            security_tier=SecurityTier.CONFIDENTIAL_FAB
        )

        # Validate lineage includes expected tables
        lineage_tables = [t['table_name'] for t in result['pipeline_context'].get('lineage', [])]
        for expected_table in ticket['expected_tables']:
            assert any(expected_table in table for table in lineage_tables), \
                f"Expected table {expected_table} not in lineage: {lineage_tables}"

        # Validate relevant procedures found
        proc_names = [p['procedure_name'] for p in result['pipeline_context'].get('relevant_procedures', [])]
        for expected_proc in ticket['expected_procedures']:
            assert any(expected_proc in proc for proc in proc_names), \
                f"Expected procedure {expected_proc} not found: {proc_names}"

        # Validate analysis mentions root cause
        analysis_text = result['analysis']
        assert ticket['expected_root_cause'].lower() in analysis_text.lower() or \
               any(word in analysis_text.lower() for word in ticket['expected_root_cause'].split()), \
               f"Root cause not mentioned in analysis"

        print(f"✓ Ticket {ticket['ticket_id']} resolved correctly")


@pytest.mark.manual
def test_resolution_time_comparison():
    """
    Manual test: Compare resolution time with/without v1.2.

    Procedure:
    1. Take 3 real tickets
    2. Time manual resolution (code review, grep, trace)
    3. Time v1.2 resolution (run pipeline debugger)
    4. Calculate speedup

    Expected: 5-10x faster with v1.2
    """
    # This is a manual test - document results in comments
    pass
```

#### 2. Performance Benchmarks

**File:** `tests/pipeline/test_performance.py`

```python
import pytest
import time
from atlassemi.pipeline.dependency_graph import DependencyGraph
from atlassemi.pipeline.code_index import SQLCodeIndex


@pytest.mark.performance
def test_lineage_query_performance(dependency_graph):
    """Lineage queries should complete in <3 seconds."""

    test_cases = [
        ('sql.dbo.tblChamberMetrics', 'downstream'),
        ('powerbi.report.yield_dashboard', 'upstream'),
        ('sql.dbo.tblWaferYield', 'downstream')
    ]

    for node_id, direction in test_cases:
        start = time.time()

        if direction == 'downstream':
            result = dependency_graph.find_downstream_impact(node_id)
        else:
            result = dependency_graph.find_upstream_lineage(node_id)

        elapsed = time.time() - start

        assert elapsed < 3.0, f"Query took {elapsed:.2f}s (limit: 3s)"
        print(f"✓ {direction} query for {node_id}: {elapsed:.2f}s ({len(result)} nodes)")


@pytest.mark.performance
def test_semantic_search_performance(code_index):
    """Semantic search should return results in <2 seconds."""

    queries = [
        "calculate Cpk for chambers",
        "aggregate defect counts by lot",
        "tool availability percentage"
    ]

    for query in queries:
        start = time.time()
        results = code_index.semantic_search(query, top_k=10)
        elapsed = time.time() - start

        assert elapsed < 2.0, f"Search took {elapsed:.2f}s (limit: 2s)"
        assert len(results) > 0, f"No results for query: {query}"
        print(f"✓ Search '{query}': {elapsed:.2f}s ({len(results)} results)")
```

#### 3. Documentation

**File:** `docs/PIPELINE_DEBUGGING.md`

Create user guide for data scientists:
- How to run metadata extraction
- How to query lineage
- How to use semantic code search
- How to integrate with ATLASsemi
- Example workflows

### Success Criteria

#### Automated Verification:
- [ ] All unit tests pass: `pytest tests/pipeline/ -v`
- [ ] Integration tests pass: `pytest tests/pipeline/ -m integration`
- [ ] Performance benchmarks pass: `pytest tests/pipeline/ -m performance`
- [ ] Type checking passes: `mypy src/atlassemi/`
- [ ] Code coverage >75%: `pytest --cov=atlassemi.pipeline --cov-report=html`

#### Manual Verification:
- [ ] Test with 3 real production tickets: Verify correct lineage and procedures identified
- [ ] Resolution time comparison: Document actual speedup (target 5-10x)
- [ ] Data scientist review: Get feedback from new hire on usability
- [ ] Manager demo: Present capabilities to manager for approval
- [ ] No false positives: Verify semantic search relevance on 10 queries

**Implementation Note:** After Phase 6, document actual resolution times for 3 tickets and compare to manual baseline.

---

## Testing Strategy

### Unit Tests

**Scope:** Test individual components in isolation.

| Component | Test File | Key Tests |
|-----------|-----------|-----------|
| SQL Metadata Extractor | `test_sql_metadata_extractor.py` | Connection, object extraction, dependencies |
| Power BI Scanner | `test_powerbi_scanner.py` | Authentication, scan initiation, result parsing |
| SQL Code Chunker | `test_sql_code_chunker.py` | Chunk boundaries, size limits, overlap |
| Code Embedder | `test_sql_code_embedder.py` | Embedding generation, model selection |
| Code Index | `test_code_index.py` | Indexing, semantic search, filters |
| Dependency Graph | `test_dependency_graph.py` | Graph construction, traversal, queries |
| Query Interface | `test_query_interface.py` | High-level queries, result formatting |
| Pipeline Context | `test_pipeline_context.py` | Entity extraction, context building |

### Integration Tests

**Scope:** Test end-to-end workflows.

1. **Full Metadata Extraction:**
   - Extract SQL metadata → verify completeness
   - Extract Power BI lineage → verify report mapping
   - Build dependency graph → verify graph structure

2. **Semantic Search Workflow:**
   - Index code → embed chunks → search → verify relevance

3. **Pipeline Debugging Workflow:**
   - Input problem → build context → analyze → verify 8D mapping

### Manual Testing Steps

1. **Metadata Accuracy:**
   - Compare extracted metadata to actual database (spot check 10 objects)
   - Verify Power BI lineage matches actual reports (verify 5 reports)

2. **Search Relevance:**
   - Run 10 semantic search queries
   - Manually verify top 5 results are relevant for each query

3. **Production Ticket Resolution:**
   - Take 3 real tickets
   - Time manual resolution (baseline)
   - Time v1.2 resolution
   - Compare results and speed

4. **User Acceptance:**
   - Data scientist uses system for 1 week
   - Collect feedback on usability
   - Iterate based on feedback

---

## Performance Considerations

### Expected Performance

| Operation | Target Time | Notes |
|-----------|-------------|-------|
| SQL metadata extraction | <2 minutes | Full database scan |
| Power BI scan | 2-5 minutes | Depends on workspace count |
| Code embedding | ~1 sec/procedure | Batch processing recommended |
| Semantic search | <2 seconds | Top 10 results |
| Lineage query | <3 seconds | Downstream/upstream traversal |
| End-to-end analysis | <30 seconds | Full DATA_PIPELINE workflow |

### Optimization Strategies

1. **Metadata Caching:**
   - Cache SQL metadata (refresh daily)
   - Cache Power BI lineage (refresh on-demand)
   - Incremental updates when possible

2. **Embedding Batch Processing:**
   - Embed all procedures once at setup
   - Re-embed only on code changes
   - Use local model (sentence-transformers) for speed

3. **Graph Indexing:**
   - Consider Neo4j if >10,000 nodes
   - For now, in-memory graph sufficient (<1000 nodes)

4. **ChromaDB Tuning:**
   - Use persistent storage (avoid re-indexing)
   - Batch insertions (100+ chunks at a time)
   - Filter queries with metadata for speed

---

## Migration Notes

### Initial Setup

1. **Install Dependencies:**
   ```bash
   pip install pyodbc chromadb sentence-transformers requests
   ```

2. **Configure SQL Server Connection:**
   - Obtain connection string (Windows Authentication or SQL Auth)
   - Test connection: `python -m atlassemi.pipeline extract-sql --connection-string "..."`

3. **Configure Power BI Access:**
   - Create Azure AD service principal (requires admin)
   - Grant Power BI admin API permissions
   - Test authentication: `python -m atlassemi.pipeline extract-powerbi --tenant-id "..."`

4. **Extract Initial Metadata:**
   ```bash
   # SQL Server
   python -m atlassemi.pipeline extract-sql \
     --connection-string "DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=FabDB;Trusted_Connection=yes;" \
     --output pipeline_metadata/sql_metadata.json

   # Power BI
   python -m atlassemi.pipeline extract-powerbi \
     --tenant-id "..." \
     --client-id "..." \
     --client-secret "..." \
     --output pipeline_metadata/powerbi_lineage.json
   ```

5. **Build Indexes:**
   ```python
   # Python script to build initial indexes
   from atlassemi.pipeline import build_pipeline_indexes

   build_pipeline_indexes(
       sql_metadata_path='pipeline_metadata/sql_metadata.json',
       powerbi_lineage_path='pipeline_metadata/powerbi_lineage.json'
   )
   ```

### Ongoing Maintenance

- **Metadata refresh:** Weekly or on-demand (after schema changes)
- **Code re-indexing:** After stored procedure updates
- **Graph updates:** Automatic when metadata refreshed

---

## Security Considerations

### Tier 2 (Confidential Fab) Requirements

**v1.2 handles confidential fab data - must follow Tier 2 security:**

1. **No Cloud Embeddings:**
   - Use sentence-transformers (local model)
   - SQL Server 2025 native vectors (if available)
   - Never send SQL code to OpenAI/Anthropic

2. **Local Storage Only:**
   - ChromaDB persistent directory on-prem
   - Metadata JSON files on secure file share
   - No cloud vector databases

3. **Access Control:**
   - Pipeline metadata accessible only to authorized data scientists
   - Power BI service principal limited to read-only
   - SQL Server connection: read-only database user

4. **Audit Trail:**
   - Log all metadata extractions
   - Log all semantic searches (who queried what)
   - Track which tickets used pipeline debugger

5. **Data Lifecycle:**
   - Metadata refreshes overwrite previous versions
   - No long-term retention of raw SQL code in logs
   - ChromaDB collection persists (approved for Tier 2)

---

## References

**SQL Server Metadata:**
- [SQL Server Metadata for Stored Procedures, Functions and Views](https://www.mssqltips.com/sqlservertip/8092/sql-server-metadata-stored-procedures-functions-views/)
- [Making SQL Server metadata queries easier with these new Views](https://www.mssqltips.com/sqlservertip/3449/making-sql-server-metadata-queries-easier-with-these-new-views/)
- [Find Stored Procedures Accessing a Table in SQL Server](https://tech-champion.com/data-science/find-stored-procedures-accessing-a-table-in-sql-server/)

**Power BI REST API:**
- [Data lineage - Power BI | Microsoft Learn](https://learn.microsoft.com/en-us/power-bi/collaborate-share/service-data-lineage)
- [Complete Semantic Model Lineage in the Power BI Service](https://en.brunner.bi/post/complete-semantic-model-lineage-in-the-power-bi-service)

**SQL Code Embedding:**
- [SQL-Server 2025: Generate Data Embeddings for Semantic Search](https://www.dbi-services.com/blog/sql-server-2025-generate-data-embeddings-for-semantic-search/)
- [Chunking Strategies for LLM Applications | Pinecone](https://www.pinecone.io/learn/chunking-strategies/)

**Graph Databases:**
- [What Is Data Lineage? Tracking Data Through Enterprise Systems - Graph Database & Analytics](https://neo4j.com/blog/graph-database/what-is-data-lineage/)
- [Neo4j Alternative: What are My Open-source Database Options?](https://memgraph.com/blog/neo4j-alternative-what-are-my-open-source-db-options)

**Related Plans:**
- v1.0: `thoughts/shared/plans/2026-01-07-orchestrator-test-suite-mvp.md`
- v1.1: `thoughts/shared/plans/2026-01-08-v1.1-rag-integration.md`

---

**Plan Status:** Ready for validation
**Target Start Date:** When new data scientist onboards
**Estimated Duration:** 25-35 hours (6 phases)
**Business Priority:** HIGH (production org velocity, boss approval)
